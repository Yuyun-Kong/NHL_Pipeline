{"cells":[{"cell_type":"markdown","source":["## game.csv\n","Reads NHL game data from a CSV file and performs data cleaning and validation. It standardizes column types, removes duplicates, checks for missing or invalid records (like incorrect game_id formats or negative scores). Finally, it saves the cleaned DataFrame to the Silver layer in the Lakehouse.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c0aa8640-ca05-4dab-ab1c-34a38f77734c"},{"cell_type":"code","source":["from pyspark.sql.functions import col, count, when, lit, to_timestamp, length, substring, trim, upper\n","from pyspark.sql.types import IntegerType, StringType\n","\n","game_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/game.csv\")\n","# df now is a Spark DataFrame containing CSV data from \"Files/Bronze(raw datasets)/game.csv\".\n","display(game_df)\n","\n","# Step 1: Check initial schema and sample data\n","print(\"Initial Schema:\")\n","game_df.printSchema()\n","game_df.show(5, truncate=False)\n","\n","# Step 2: Check for data consistency (data types)\n","# Cast columns to appropriate types\n","game_df = game_df.withColumn(\"game_id\", col(\"game_id\").cast(StringType())) \\\n","       .withColumn(\"season\", col(\"season\").cast(StringType())) \\\n","       .withColumn(\"type\", upper(trim(col(\"type\"))).cast(StringType())) \\\n","       .withColumn(\"date_time_GMT\", to_timestamp(col(\"date_time_GMT\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\")) \\\n","       .withColumn(\"away_team_id\", col(\"away_team_id\").cast(StringType())) \\\n","       .withColumn(\"home_team_id\", col(\"home_team_id\").cast(StringType())) \\\n","       .withColumn(\"away_goals\", col(\"away_goals\").cast(IntegerType())) \\\n","       .withColumn(\"home_goals\", col(\"home_goals\").cast(IntegerType())) \\\n","       .withColumn(\"outcome\", col(\"outcome\").cast(StringType())) \\\n","       .withColumn(\"home_rink_side_start\", col(\"home_rink_side_start\").cast(StringType())) \\\n","       .withColumn(\"venue\", col(\"venue\").cast(StringType())) \\\n","       .withColumn(\"venue_link\", col(\"venue_link\").cast(StringType())) \\\n","       .withColumn(\"venue_time_zone_id\", col(\"venue_time_zone_id\").cast(StringType())) \\\n","       .withColumn(\"venue_time_zone_offset\", col(\"venue_time_zone_offset\").cast(IntegerType())) \\\n","       .withColumn(\"venue_time_zone_tz\", col(\"venue_time_zone_tz\").cast(StringType()))\n","\n","print(\"Schema after type casting:\")\n","game_df.printSchema()\n","\n","# Step 3: Check for duplicates\n","# 3.1: Check for exact duplicates (all columns identical)\n","exact_duplicates = game_df.groupBy(game_df.columns).count().filter(col(\"count\") > 1)\n","exact_duplicate_count = exact_duplicates.count()\n","print(f\"Number of exact duplicate groups: {exact_duplicate_count}\")\n","\n","# Show some exact duplicates for inspection\n","print(\"Sample of exact duplicate groups:\")\n","exact_duplicates.show(5, truncate=False)\n","\n","# Decision: Drop exact duplicates (exact replicas likely errors)\n","if exact_duplicate_count > 0:\n","    print(\"Dropping exact duplicate rows...\")\n","    game_df = game_df.dropDuplicates()\n","    print(f\"Rows after dropping exact duplicates: {game_df.count()}\")\n","\n","# 3.2: Check for game_id duplicates (non-exact duplicates)\n","# Each game_id should be unique in game.csv\n","game_id_duplicates = game_df.groupBy(\"game_id\").count().filter(col(\"count\") > 1)\n","game_id_duplicate_count = game_id_duplicates.count()\n","print(f\"Number of game_id duplicate groups: {game_id_duplicate_count}\")\n","\n","# Show game_id duplicates for inspection\n","if game_id_duplicate_count > 0:\n","    print(\"Sample of game_id duplicate groups:\")\n","    game_id_duplicates.show(5, truncate=False)\n","\n","    # Decision: Drop game_id duplicates, keeping first occurrence\n","    print(\"Dropping duplicate game_ids, keeping first occurrence...\")\n","    game_df = game_df.dropDuplicates([\"game_id\"])\n","    print(f\"Rows after dropping game_id duplicates: {game_df.count()}\")\n","\n","# Step 4: Check for missing values\n","missing_summary = game_df.select([count(when(col(c).isNull(), c)).alias(c) for c in game_df.columns])\n","missing_summary.show()\n","\n","# Decision: leave some nulls (like home_rink_side_start) as-is for Silver\n","\n","# Step 5: Check for invalid records\n","# Validate game_id: 10 digits (4-digit season 2000-2019, 2-digit type, 4-digit numeric game number)\n","# Cross-check game_id type with type column (A=04, R=02, P=03)\n","# away_goals and home_goals: ensure >= 0\n","stats = game_df.describe([\"away_goals\", \"home_goals\"])\n","stats.show()\n","\n","# Step 5: Check for invalid records\n","# Validate game_id: 10 digits (4-digit season 2000-2019, 2-digit type, 4-digit numeric game number)\n","# Cross-check game_id type with type column (A=04, R=02, P=03), allow '01' preseason\n","invalid_game_id = game_df.filter(\n","    (length(col(\"game_id\")) != 10) | \n","    (substring(col(\"game_id\"), 1, 4).cast(IntegerType()) < 2000) | \n","    (substring(col(\"game_id\"), 1, 4).cast(IntegerType()) > 2019) | \n","    (~col(\"game_id\").rlike(r'^\\d{10}$')) |\n","    (~substring(col(\"game_id\"), 5, 2).isin([\"01\", \"02\", \"03\", \"04\"]))\n",")\n","invalid_game_id_count = invalid_game_id.count()\n","print(f\"Invalid game_id (length, year, or format): {invalid_game_id_count}\")\n","\n","if invalid_game_id_count > 0:\n","    print(\"Sample invalid game_id rows (format issues):\")\n","    invalid_game_id.show(5, truncate=False)\n","\n","# Instead of dropping type mismatches, flag them\n","game_df = game_df.withColumn(\n","    \"type_mismatch_flag\",\n","    when(\n","        ((col(\"type\") == \"A\") & (substring(col(\"game_id\"), 5, 2) != \"04\")) |\n","        ((col(\"type\") == \"R\") & (substring(col(\"game_id\"), 5, 2) != \"02\")) |\n","        ((col(\"type\") == \"P\") & (substring(col(\"game_id\"), 5, 2) != \"03\")),\n","        lit(1)\n","    ).otherwise(lit(0))\n",")\n","\n","mismatch_count = game_df.filter(col(\"type_mismatch_flag\") == 1).count()\n","print(f\"Number of type mismatches flagged: {mismatch_count}\")\n","game_df.filter(col(\"type_mismatch_flag\") == 1).show(5, truncate=False)\n","\n","# Check for invalid goals (negative values)\n","invalid_goals = game_df.filter((col(\"away_goals\") < 0) | (col(\"home_goals\") < 0))\n","invalid_goals_count = invalid_goals.count()\n","print(f\"Invalid goals (negative): {invalid_goals_count}\")\n","if invalid_goals_count > 0:\n","    print(\"Dropping rows with negative goals...\")\n","    game_df = game_df.filter((col(\"away_goals\") >= 0) & (col(\"home_goals\") >= 0))\n","    print(f\"Rows after dropping invalid goals: {game_df.count()}\")\n","\n","# Step 6: Final cleaned DataFrame\n","print(\"Final row count after cleaning: \" + str(game_df.count()))\n","game_df.show(5, truncate=False)\n","\n","# Step 7: Save cleaned DataFrame to Lakehouse (Silver layer)\n","(\n","    game_df.write\n","    .mode(\"overwrite\")  \n","    .saveAsTable(\"silver_game\")\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6f817e0c-a819-4df2-a1a6-700b1a0c3f56"},{"cell_type":"markdown","source":["## game_plays_players.csv\n","Loads player game data from a CSV file and performs comprehensive data cleaning. It standardizes column types, removes duplicates, and filters out invalid game_id, player_id, and play_id values based on strict format and consistency rules. Finally, the cleaned dataset is saved to the Silver layer in the Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4189c51f-2f5a-4e32-ba6a-da8e32685f5a"},{"cell_type":"code","source":["from pyspark.sql.functions import col, count, when, lit, length, substring, regexp_extract\n","from pyspark.sql.types import IntegerType, StringType\n","\n","players_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/game_plays_players.csv\")\n","# df now is a Spark DataFrame containing CSV data from \"Files/Bronze(raw datasets)/game_plays_players.csv\".\n","display(players_df)\n","\n","# Step 1: Show initial schema and sample data\n","print(\"Initial Schema:\")\n","players_df.printSchema()\n","players_df.show(5, truncate=False)\n","\n","# Step 2: Check for data consistency (data types)\n","# Cast columns to appropriate types based on the profiling report\n","players_df = players_df.withColumn(\"play_id\", col(\"play_id\").cast(StringType())) \\\n","                       .withColumn(\"game_id\", col(\"game_id\").cast(StringType())) \\\n","                       .withColumn(\"player_id\", col(\"player_id\").cast(StringType())) \\\n","                       .withColumn(\"playerType\", col(\"playerType\").cast(StringType()))\n","\n","print(\"Schema after type casting:\")\n","players_df.printSchema()\n","\n","# Step 3: Check for duplicates\n","exact_duplicates = players_df.groupBy(players_df.columns).count().filter(col(\"count\") > 1)\n","exact_duplicate_count = exact_duplicates.count()\n","print(f\"Number of exact duplicate groups: {exact_duplicate_count}\")\n","\n","# Show some exact duplicates for inspection\n","print(\"Sample of exact duplicate groups:\")\n","exact_duplicates.show(5, truncate=False)\n","\n","# Show full rows for exact duplicates\n","if exact_duplicate_count > 0:\n","    exact_duplicate_keys = exact_duplicates.drop(\"count\")\n","    exact_duplicate_rows = players_df.join(exact_duplicate_keys, players_df.columns, \"inner\")\n","    print(\"Full rows of exact duplicates:\")\n","    exact_duplicate_rows.show(10, truncate=False)\n","\n","# Decision: Drop exact duplicates (exact replicas likely errors)\n","if exact_duplicate_count > 0:\n","    print(\"Dropping exact duplicate rows...\")\n","    players_df = players_df.dropDuplicates()\n","    print(f\"Rows after dropping exact duplicates: {players_df.count()}\")\n","\n","# Step 4: Check for missing values\n","# Report shows no missing values (0.0%)\n","missing_summary = players_df.select([count(when(col(c).isNull(), c)).alias(c) for c in players_df.columns])\n","missing_summary.show()\n","\n","# Step 5: Check for invalid records\n","# player_id: ensure >= 1\n","stats = players_df.describe([\"player_id\"])\n","stats.show()\n","\n","# Validate game_id format: 10 digits (4-digit season 2000–2019, 2-digit type, 4-digit game number)\n","invalid_game_id = players_df.filter(\n","    (length(col(\"game_id\")) != 10) | \n","    (substring(col(\"game_id\"), 1, 4).cast(IntegerType()) < 2000) | \n","    (substring(col(\"game_id\"), 1, 4).cast(IntegerType()) > 2019) | \n","    (~col(\"game_id\").rlike(r'^\\d{10}$')) |\n","    (~substring(col(\"game_id\"), 5, 2).isin([\"01\", \"02\", \"03\", \"04\"]))\n",")\n","invalid_game_id_count = invalid_game_id.count()\n","print(f\"Invalid game_id (format, season, or type): {invalid_game_id_count}\")\n","if invalid_game_id_count > 0:\n","    print(\"Invalid game_id rows:\")\n","    invalid_game_id.show(5, truncate=False)\n","    print(\"Dropping rows with invalid game_id...\")\n","    players_df = players_df.filter(\n","        (length(col(\"game_id\")) == 10) & \n","        (substring(col(\"game_id\"), 1, 4).cast(IntegerType()) >= 2000) & \n","        (substring(col(\"game_id\"), 1, 4).cast(IntegerType()) <= 2019) & \n","        (col(\"game_id\").rlike(r'^\\d{10}$')) &\n","        (substring(col(\"game_id\"), 5, 2).isin([\"01\", \"02\", \"03\", \"04\"]))\n","    )\n","    print(f\"Rows after dropping invalid game_id: {players_df.count()}\")\n","\n","# Validate player_id: exactly 7 digits (string of 7 numeric chars)\n","invalid_player_id = players_df.filter(~col(\"player_id\").rlike(r'^\\d{7}$'))\n","invalid_player_id_count = invalid_player_id.count()\n","print(f\"Invalid player_id count (not 7 digits): {invalid_player_id_count}\")\n","if invalid_player_id_count > 0:\n","    print(\"Invalid player_id rows:\")\n","    invalid_player_id.show(5, truncate=False)\n","    print(\"Dropping rows with invalid player_id...\")\n","    players_df = players_df.filter(col(\"player_id\").rlike(r'^\\d{7}$'))\n","    print(f\"Rows after dropping invalid player_id: {players_df.count()}\")\n","\n","#Validate play_id: game id + '_' + numeric number\n","play_id_invalid = players_df.filter(\n","    (~col(\"play_id\").rlike(r'^\\d{10}_\\d+$')) |                     # Must be 10 digits + underscore + digits\n","    (regexp_extract(col(\"play_id\"), r'^(\\d{10})_', 1) != col(\"game_id\"))  # The first 10 digits before underscore must match game_id\n",")\n","\n","invalid_play_id_count = play_id_invalid.count()\n","print(f\"Invalid play_id count (format not game_id + _number): {invalid_play_id_count}\")\n","\n","if invalid_play_id_count > 0:\n","    print(\"Invalid play_id rows:\")\n","    play_id_invalid.show(5, truncate=False)\n","    print(\"Dropping rows with invalid play_id...\")\n","    players_df = players_df.filter(\n","        (col(\"play_id\").rlike(r'^\\d{10}_\\d+$')) &\n","        (regexp_extract(col(\"play_id\"), r'^(\\d{10})_', 1) == col(\"game_id\"))\n","    )\n","    print(f\"Rows after dropping invalid play_id: {players_df.count()}\")\n","\n","# Step 6: Final cleaned DataFrame\n","print(\"Final row count after cleaning: \" + str(players_df.count()))\n","players_df.show(5, truncate=False)\n","\n","# Step 7: Save cleaned DataFrame to Lakehouse (Silver layer)\n","(\n","    players_df.write\n","    .mode(\"overwrite\")  \n","    .saveAsTable(\"silver_game_plays_players\")\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"90f906cf-dabf-4f7f-8cf0-67ec533fa617"},{"cell_type":"markdown","source":["## game_scratches.csv\n","Loads player scratch data from a CSV file and performs data cleaning and validation. It standardizes column types, removes duplicates, checks for missing values, and filters out invalid game_id, player_id, and team_id entries. Finally, the cleaned dataset is saved to the Silver layer in the Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f230b60-f90f-4c57-9694-261eebb407b3"},{"cell_type":"code","source":["from pyspark.sql.functions import col, count, when, lit, length, substring, regexp_extract\n","from pyspark.sql.types import IntegerType, StringType\n","\n","scratches_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/game_scratches.csv\")\n","# df now is a Spark DataFrame containing CSV data from \"Files/Bronze(raw datasets)/game_scratches.csv\".\n","display(scratches_df)\n","\n","# Step 1: Check initial schema and sample data\n","print(\"Initial Schema:\")\n","scratches_df.printSchema()\n","scratches_df.show(5, truncate=False)\n","\n","# Step 2: Check for data consistency (data types)\n","# game_id as StringType (large identifier), team_id and player_id as IntegerType\n","scratches_df = scratches_df.withColumn(\"game_id\", col(\"game_id\").cast(StringType())) \\\n","                           .withColumn(\"team_id\", col(\"team_id\").cast(StringType())) \\\n","                           .withColumn(\"player_id\", col(\"player_id\").cast(StringType()))\n","\n","print(\"Schema after type casting:\")\n","scratches_df.printSchema()\n","\n","# Step 3: Check for duplicates\n","exact_duplicates = scratches_df.groupBy(scratches_df.columns).count().filter(col(\"count\") > 1)\n","exact_duplicate_count = exact_duplicates.count()\n","print(f\"Number of exact duplicate groups: {exact_duplicate_count}\")\n","\n","# Show some exact duplicates for inspection\n","print(\"Sample of exact duplicate groups:\")\n","exact_duplicates.show(5, truncate=False)\n","\n","# Decision: Drop exact duplicates (exact replicas likely errors)\n","if exact_duplicate_count > 0:\n","    print(\"Dropping exact duplicate rows...\")\n","    scratches_df = scratches_df.dropDuplicates()\n","    print(f\"Rows after dropping exact duplicates: {scratches_df.count()}\")\n","\n","# Step 4: Check for missing values\n","missing_summary = scratches_df.select([count(when(col(c).isNull(), c)).alias(c) for c in scratches_df.columns])\n","missing_summary.show()\n","\n","# Step 5: Check for invalid records\n","stats = scratches_df.describe([\"team_id\", \"player_id\"])\n","stats.show()\n","\n","# Validate game_id format: 10 digits (4-digit season, 2-digit type, 4-digit game number)\n","invalid_game_id = scratches_df.filter(\n","    (length(col(\"game_id\")) != 10) | \n","    (substring(col(\"game_id\"), 1, 4).cast(IntegerType()) < 2000) | \n","    (substring(col(\"game_id\"), 1, 4).cast(IntegerType()) > 2019) | \n","    (~col(\"game_id\").rlike(r'^\\d{10}$'))\n",")\n","\n","invalid_game_id_count = invalid_game_id.count()\n","print(f\"Invalid game_id (not 10 digits or invalid season): {invalid_game_id_count}\")\n","if invalid_game_id_count > 0:\n","    print(\"Invalid game_id rows:\")\n","    invalid_game_id.show(5, truncate=False)\n","    print(\"Dropping rows with invalid game_id...\")\n","    scratches_df = scratches_df.filter(\n","        (length(col(\"game_id\")) == 10) & \n","        (substring(col(\"game_id\"), 1, 4).cast(IntegerType()) >= 2000) & \n","        (substring(col(\"game_id\"), 1, 4).cast(IntegerType()) <= 2019) & \n","        (col(\"game_id\").cast(StringType()).regexp_match(r'^\\d{10}$'))\n","    )\n","    print(f\"Rows after dropping invalid game_id: {scratches_df.count()}\")\n","\n","# Validate player_id format: must be exactly 7 digits\n","invalid_player_id = scratches_df.filter(~col(\"player_id\").rlike(r'^\\d{7}$'))\n","invalid_player_id_count = invalid_player_id.count()\n","print(f\"Invalid player_id (not exactly 7 digits): {invalid_player_id_count}\")\n","if invalid_player_id_count > 0:\n","    print(\"Invalid player_id rows:\")\n","    invalid_player_id.show(5, truncate=False)\n","    print(\"Dropping rows with invalid player_id...\")\n","    scratches_df = scratches_df.filter(col(\"player_id\").rlike(r'^\\d{7}$'))\n","    print(f\"Rows after dropping invalid player_id: {scratches_df.count()}\")\n","\n","# Validate team_id: numeric and >= 1\n","invalid_team_id = scratches_df.filter(\n","    (~col(\"team_id\").rlike(r'^\\d+$')) | \n","    (col(\"team_id\").cast(IntegerType()) < 1)\n",")\n","invalid_team_id_count = invalid_team_id.count()\n","print(f\"Invalid team_id (not numeric or less than 1): {invalid_team_id_count}\")\n","if invalid_team_id_count > 0:\n","    print(\"Invalid team_id rows:\")\n","    invalid_team_id.show(5, truncate=False)\n","    print(\"Dropping rows with invalid team_id...\")\n","    scratches_df = scratches_df.filter(\n","        (col(\"team_id\").rlike(r'^\\d+$')) & \n","        (col(\"team_id\").cast(IntegerType()) >= 1)\n","    )\n","    print(f\"Rows after dropping invalid team_id: {scratches_df.count()}\")\n","\n","# Step 6: Final cleaned DataFrame\n","print(\"Final row count after cleaning: \" + str(scratches_df.count()))\n","scratches_df.show(5, truncate=False)\n","\n","# Step 7: Save cleaned DataFrame to Lakehouse (Silver layer)\n","(\n","    scratches_df.write\n","    .mode(\"overwrite\")  \n","    .saveAsTable(\"silver_scratches\")\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3a22a9c9-06eb-424d-8fa7-986c9d868bd9"},{"cell_type":"markdown","source":["## team_info.csv\n","Loads team information data from a CSV file and performs cleaning and validation. It standardizes column types, removes duplicates and invalid records such as empty fields, negative IDs, or incorrect link formats. Finally, it saves the cleaned dataset to the Silver layer in the Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52e068b3-b8d7-4392-9e5f-e19f88c3ad51"},{"cell_type":"code","source":["from pyspark.sql.functions import col, count, when, length, trim\n","from pyspark.sql.types import IntegerType, StringType\n","\n","team_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/team_info.csv\")\n","# df now is a Spark DataFrame containing CSV data from \"Files/Bronze(raw datasets)/team_info.csv\".\n","display(team_df)\n","\n","# Step 1: Show initial schema and sample data\n","print(\"Initial Schema:\")\n","team_df.printSchema()\n","team_df.show(5, truncate=False)\n","\n","# Step 2: Check for data consistency (data types)\n","# Cast columns to appropriate types \n","team_df = team_df.withColumn(\"team_id\", col(\"team_id\").cast(StringType())) \\\n","                 .withColumn(\"franchiseId\", col(\"franchiseId\").cast(StringType())) \\\n","                 .withColumn(\"shortName\", col(\"shortName\").cast(StringType())) \\\n","                 .withColumn(\"teamName\", col(\"teamName\").cast(StringType())) \\\n","                 .withColumn(\"abbreviation\", col(\"abbreviation\").cast(StringType())) \\\n","                 .withColumn(\"link\", col(\"link\").cast(StringType()))\n","\n","print(\"Schema after type casting:\")\n","team_df.printSchema()\n","\n","# Step 3: Check for duplicates\n","exact_duplicates = team_df.groupBy(team_df.columns).count().filter(col(\"count\") > 1)\n","exact_duplicate_count = exact_duplicates.count()\n","print(f\"Number of exact duplicate groups: {exact_duplicate_count}\")\n","\n","# Show some exact duplicates for inspection (if any)\n","if exact_duplicate_count > 0:\n","    print(\"Sample of exact duplicate groups:\")\n","    exact_duplicates.show(5, truncate=False)\n","\n","# Decision: Drop exact duplicates (exact replicas likely errors)\n","if exact_duplicate_count > 0:\n","    print(\"Dropping exact duplicate rows...\")\n","    team_df = team_df.dropDuplicates()\n","    print(f\"Rows after dropping exact duplicates: {team_df.count()}\")\n","\n","# Check for team id duplicates\n","team_id_duplicates = team_df.groupBy(\"team_id\").count().filter(col(\"count\") > 1)\n","team_id_duplicate_count = team_id_duplicates.count()\n","print(f\"Number of team_id duplicate groups: {team_id_duplicate_count}\")\n","\n","# Show game_id duplicates for inspection\n","if team_id_duplicate_count > 0:\n","    print(\"Sample of game_id duplicate groups:\")\n","    team_id_duplicates.show(5, truncate=False)\n","\n","    # Decision: Drop game_id duplicates, keeping first occurrence\n","    print(\"Dropping duplicate game_ids, keeping first occurrence...\")\n","    team_df = team_df.dropDuplicates([\"team_id\"])\n","    print(f\"Rows after dropping game_id duplicates: {team_df.count()}\")\n","\n","# Step 4: Check for missing values\n","\n","missing_summary = team_df.select([count(when(col(c).isNull(), c)).alias(c) for c in team_df.columns])\n","missing_summary.show()\n","\n","# Step 5: Check for invalid records\n","# Validate team_id and franchiseId: ensure >= 1 (no negatives or zero)\n","# shortName, teamName, abbreviation: ensure non-empty strings\n","# link: ensure starts with \"/api/v1/teams/\"\n","stats = team_df.describe([\"team_id\", \"franchiseId\"])\n","stats.show()\n","\n","# Check for invalid team_id or franchiseId (negative or zero)\n","invalid_ids = team_df.filter((col(\"team_id\") < 1) | (col(\"franchiseId\") < 1))\n","invalid_ids_count = invalid_ids.count()\n","print(f\"Invalid IDs (negative or zero): {invalid_ids_count}\")\n","if invalid_ids_count > 0:\n","    print(\"Dropping rows with invalid IDs...\")\n","    team_df = team_df.filter((col(\"team_id\") >= 1) & (col(\"franchiseId\") >= 1))\n","    print(f\"Rows after dropping invalid IDs: {team_df.count()}\")\n","\n","# Check for empty strings in shortName, teamName, abbreviation\n","invalid_strings = team_df.filter(\n","    (length(col(\"shortName\")) == 0) | \n","    (length(col(\"teamName\")) == 0) | \n","    (length(col(\"abbreviation\")) == 0)\n",")\n","invalid_strings_count = invalid_strings.count()\n","print(f\"Empty string fields: {invalid_strings_count}\")\n","if invalid_strings_count > 0:\n","    print(\"Dropping rows with empty string fields...\")\n","    team_df = team_df.filter(\n","        (length(col(\"shortName\")) > 0) & \n","        (length(col(\"teamName\")) > 0) & \n","        (length(col(\"abbreviation\")) > 0)\n","    )\n","    print(f\"Rows after dropping empty strings: {team_df.count()}\")\n","\n","# Check for invalid link format (should start with \"/api/v1/teams/\")\n","invalid_links = team_df.filter(~col(\"link\").startswith(\"/api/v1/teams/\"))\n","invalid_links_count = invalid_links.count()\n","print(f\"Invalid links (not starting with '/api/v1/teams/'): {invalid_links_count}\")\n","if invalid_links_count > 0:\n","    print(\"Invalid link rows:\")\n","    invalid_links.show(5, truncate=False)\n","    print(\"Dropping rows with invalid links...\")\n","    team_df = team_df.filter(col(\"link\").startswith(\"/api/v1/teams/\"))\n","    print(f\"Rows after dropping invalid links: {team_df.count()}\")\n","\n","# Step 6: Final cleaned DataFrame\n","print(\"Final row count after cleaning: \" + str(team_df.count()))\n","team_df.show(5, truncate=False)# Show initial schema and sample data\n","print(\"Initial Schema:\")\n","team_df.printSchema()\n","team_df.show(5, truncate=False)\n","\n","# Step 7: Save cleaned DataFrame to Lakehouse (Silver layer)\n","(\n","    team_df.write\n","    .mode(\"overwrite\")  \n","    .saveAsTable(\"silver_team_info\")\n",")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8d61fa0d-159a-44e1-b0d9-e3b0a86d574c"},{"cell_type":"markdown","source":["## game_teams_stats.csv\n","Loads game team statistics data from a CSV file, performs type casting, removes duplicate rows, and replaces negative numeric values with zero. It then generates a summary table showing row counts across cleaning steps. Finally, it saves the cleaned dataset to the Silver layer in the Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"16700630-e7ea-47dc-ad52-35d40f2bf37e"},{"cell_type":"code","source":["from pyspark.sql.functions import col, lower, trim, when, count as count_\n","from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType\n","\n","# ======================================================\n","# STEP 0: Load raw dataset\n","# ======================================================\n","raw_game_team_df = (\n","    spark.read.format(\"csv\")\n","    .option(\"header\", \"true\")\n","    .load(\"Files/Bronze(raw datasets)/game_teams_stats.csv\")\n",")\n","initial_count = raw_game_team_df.count()\n","print(f\"Initial row count: {initial_count}\")\n","\n","# Show original schema before type casting\n","original_schema = [(f.name, f.dataType.simpleString()) for f in raw_game_team_df.schema.fields]\n","schema_changes = []\n","\n","# ======================================================\n","# STEP 1: Type Casting\n","# ======================================================\n","game_team_df = (\n","    raw_game_team_df\n","    .withColumn(\"game_id\", col(\"game_id\").cast(StringType()))\n","    .withColumn(\"team_id\", col(\"team_id\").cast(IntegerType()))\n","    .withColumn(\"HoA\", lower(trim(col(\"HoA\"))))\n","    .withColumn(\"won\", col(\"won\").cast(BooleanType()))\n","    .withColumn(\"settled_in\", lower(trim(col(\"settled_in\"))))\n","    .withColumn(\"head_coach\", col(\"head_coach\").cast(StringType()))\n","    .withColumn(\"goals\", col(\"goals\").cast(IntegerType()))\n","    .withColumn(\"shots\", col(\"shots\").cast(IntegerType()))\n","    .withColumn(\"hits\", col(\"hits\").cast(IntegerType()))\n","    .withColumn(\"pim\", col(\"pim\").cast(IntegerType()))\n","    .withColumn(\"powerPlayOpportunities\", col(\"powerPlayOpportunities\").cast(IntegerType()))\n","    .withColumn(\"powerPlayGoals\", col(\"powerPlayGoals\").cast(IntegerType()))\n","    .withColumn(\"faceOffWinPercentage\", col(\"faceOffWinPercentage\").cast(FloatType()))\n","    .withColumn(\"giveaways\", col(\"giveaways\").cast(IntegerType()))\n","    .withColumn(\"takeaways\", col(\"takeaways\").cast(IntegerType()))\n","    .withColumn(\"blocked\", col(\"blocked\").cast(IntegerType()))\n","    .withColumn(\"startRinkSide\", lower(trim(col(\"startRinkSide\"))))\n",")\n","\n","count_after_type_cast = game_team_df.count()\n","\n","# Identify schema changes\n","new_schema = [(f.name, f.dataType.simpleString()) for f in game_team_df.schema.fields]\n","for (name_o, type_o), (name_n, type_n) in zip(original_schema, new_schema):\n","    if type_o != type_n:\n","        schema_changes.append((name_o, type_o, type_n))\n","\n","# ======================================================\n","# STEP 2: Drop exact duplicates\n","# ======================================================\n","count_before = count_after_type_cast\n","game_team_df = game_team_df.dropDuplicates()\n","count_after = game_team_df.count()\n","exact_dupes_dropped = count_before - count_after\n","\n","# ======================================================\n","# STEP 3: Fix negative numeric values\n","# ======================================================\n","numeric_cols = [\n","    \"goals\", \"shots\", \"hits\", \"pim\", \"powerPlayOpportunities\", \"powerPlayGoals\",\n","    \"giveaways\", \"takeaways\", \"blocked\"\n","]\n","for c in numeric_cols:\n","    game_team_df = game_team_df.withColumn(c, when(col(c) < 0, 0).otherwise(col(c)))\n","count_after_neg_fix = game_team_df.count()\n","\n","# ======================================================\n","# STEP 4: Placeholder cleaning checks (for future logic)\n","# ======================================================\n","# These are placeholders so script won’t error out\n","pair_dupes_dropped = 0\n","faceoff_dropped = 0\n","hoa_dropped = 0\n","settled_in_dropped = 0\n","startRinkSide_dropped = 0\n","logical_ppg_gt_ppo_dropped = 0\n","logical_goals_lt_ppg_dropped = 0\n","multi_winners_dropped = 0\n","\n","final_count = game_team_df.count()\n","\n","# ======================================================\n","# STEP 5: Summary Table\n","# ======================================================\n","summary_data = [\n","    (\"Initial row count (raw)\", initial_count, None),\n","    (\"After type casting\", count_after_type_cast, None),\n","    (\"Exact duplicates dropped\", exact_dupes_dropped, f\"{exact_dupes_dropped} rows dropped\"),\n","    (\"After dropping exact duplicates\", count_after, None),\n","    (\"Duplicate (game_id, team_id) dropped\", pair_dupes_dropped, f\"{pair_dupes_dropped} rows dropped\"),\n","    (\"Negative values fixed\", count_after_neg_fix, \"Negative values replaced by zero (no rows dropped)\"),\n","    (\"Invalid faceOffWinPercentage removed\", faceoff_dropped, f\"{faceoff_dropped} rows dropped\"),\n","    (\"Invalid HoA removed\", hoa_dropped, f\"{hoa_dropped} rows dropped\"),\n","    (\"Invalid settled_in removed\", settled_in_dropped, f\"{settled_in_dropped} rows dropped\"),\n","    (\"Invalid startRinkSide removed\", startRinkSide_dropped, f\"{startRinkSide_dropped} rows dropped\"),\n","    (\"powerPlayGoals > powerPlayOpportunities removed\", logical_ppg_gt_ppo_dropped, f\"{logical_ppg_gt_ppo_dropped} rows dropped\"),\n","    (\"goals < powerPlayGoals removed\", logical_goals_lt_ppg_dropped, f\"{logical_goals_lt_ppg_dropped} rows dropped\"),\n","    (\"Multiple winners removed\", multi_winners_dropped, f\"{multi_winners_dropped} rows dropped\"),\n","    (\"Final cleaned row count\", final_count, None)\n","]\n","\n","summary_df = spark.createDataFrame(summary_data, [\"Step\", \"Row Count\", \"Details\"])\n","\n","print(\"📊 Cleaning Process Summary (step-by-step):\")\n","display(summary_df)\n","\n","# ======================================================\n","# STEP 6: Preview Data\n","# ======================================================\n","print(\"🧐 Original raw data preview (5 rows):\")\n","display(raw_game_team_df.limit(5))\n","\n","print(\"✅ Final cleaned data preview (5 rows):\")\n","display(game_team_df.limit(5))\n","\n","# ======================================================\n","# STEP 7: Save to Silver Layer\n","# ======================================================\n","game_team_df.write.mode(\"overwrite\").saveAsTable(\"silver_game_team_stats\")\n","\n","print(\"✅ Successfully saved cleaned dataset to Silver layer: silver_game_team_stats\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59f2d9ff-83b0-48f5-a38a-292c2730c20b"},{"cell_type":"code","source":["from pyspark.sql.functions import col, lower, trim, when, count as count_\n","from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType\n","\n","# --- Step 0: Load raw game_teams_stats from metastore table ---\n","raw_game_team_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/game_teams_stats.csv\")\n","initial_count = raw_game_team_df.count()\n","print(f\"Initial row count: {initial_count}\")\n","\n","# Show original schema before type casting\n","original_schema = [(f.name, f.dataType.simpleString()) for f in raw_game_team_df.schema.fields]\n","schema_changes = []\n","\n","# --- Step 1: Type casting ---\n","game_team_df = raw_game_team_df \\\n","    .withColumn(\"game_id\", col(\"game_id\").cast(StringType())) \\\n","    .withColumn(\"team_id\", col(\"team_id\").cast(IntegerType())) \\\n","    .withColumn(\"HoA\", lower(trim(col(\"HoA\")))) \\\n","    .withColumn(\"won\", col(\"won\").cast(BooleanType())) \\\n","    .withColumn(\"settled_in\", lower(trim(col(\"settled_in\")))) \\\n","    .withColumn(\"head_coach\", col(\"head_coach\").cast(StringType())) \\\n","    .withColumn(\"goals\", col(\"goals\").cast(IntegerType())) \\\n","    .withColumn(\"shots\", col(\"shots\").cast(IntegerType())) \\\n","    .withColumn(\"hits\", col(\"hits\").cast(IntegerType())) \\\n","    .withColumn(\"pim\", col(\"pim\").cast(IntegerType())) \\\n","    .withColumn(\"powerPlayOpportunities\", col(\"powerPlayOpportunities\").cast(IntegerType())) \\\n","    .withColumn(\"powerPlayGoals\", col(\"powerPlayGoals\").cast(IntegerType())) \\\n","    .withColumn(\"faceOffWinPercentage\", col(\"faceOffWinPercentage\").cast(FloatType())) \\\n","    .withColumn(\"giveaways\", col(\"giveaways\").cast(IntegerType())) \\\n","    .withColumn(\"takeaways\", col(\"takeaways\").cast(IntegerType())) \\\n","    .withColumn(\"blocked\", col(\"blocked\").cast(IntegerType())) \\\n","    .withColumn(\"startRinkSide\", lower(trim(col(\"startRinkSide\"))))\n","\n","count_after_type_cast = game_team_df.count()\n","\n","# Capture schema after casting for comparison\n","new_schema = [(f.name, f.dataType.simpleString()) for f in game_team_df.schema.fields]\n","\n","# Identify schema changes\n","for (name_o, type_o), (name_n, type_n) in zip(original_schema, new_schema):\n","    if type_o != type_n:\n","        schema_changes.append((name_o, type_o, type_n))\n","\n","# Dictionary to hold samples of dropped rows per step\n","dropped_samples = {}\n","\n","# --- Step 2: Drop exact duplicates ---\n","count_before = count_after_type_cast\n","exact_duplicates_df = game_team_df.subtract(game_team_df.dropDuplicates())\n","sample_dupes = exact_duplicates_df.limit(5)\n","dropped_samples['Exact duplicates dropped'] = sample_dupes\n","game_team_df = game_team_df.dropDuplicates()\n","count_after = game_team_df.count()\n","exact_dupes_dropped = count_before - count_after\n","\n","# --- Step 3: Drop duplicate (game_id, team_id) pairs ---\n","count_before = count_after\n","pair_duplicates_df = game_team_df.subtract(game_team_df.dropDuplicates([\"game_id\", \"team_id\"]))\n","sample_pair_dupes = pair_duplicates_df.limit(5)\n","dropped_samples['Duplicate (game_id, team_id) dropped'] = sample_pair_dupes\n","game_team_df = game_team_df.dropDuplicates([\"game_id\", \"team_id\"])\n","count_after = game_team_df.count()\n","pair_dupes_dropped = count_before - count_after\n","\n","# --- Step 4: Replace negative values with zero (no rows dropped) ---\n","numeric_cols = [\n","    \"goals\", \"shots\", \"hits\", \"pim\", \"powerPlayOpportunities\", \"powerPlayGoals\",\n","    \"giveaways\", \"takeaways\", \"blocked\"\n","]\n","for c in numeric_cols:\n","    game_team_df = game_team_df.withColumn(c, when(col(c) < 0, 0).otherwise(col(c)))\n","count_after_neg_fix = game_team_df.count()  # no rows dropped, so no sample\n","\n","# --- Step 5: Filter invalid faceOffWinPercentage ---\n","count_before = game_team_df.count()\n","invalid_faceoff = game_team_df.filter(\n","    (col(\"faceOffWinPercentage\") < 0) | \n","    (col(\"faceOffWinPercentage\") > 100) |\n","    (col(\"faceOffWinPercentage\").isNull())\n",")\n","count_invalid_faceoff = invalid_faceoff.count()\n","sample_faceoff = invalid_faceoff.limit(5)\n","dropped_samples['Invalid faceOffWinPercentage removed'] = sample_faceoff\n","game_team_df = game_team_df.subtract(invalid_faceoff)\n","count_after = game_team_df.count()\n","faceoff_dropped = count_before - count_after\n","\n","# --- Step 6: Filter invalid HoA ---\n","count_before = game_team_df.count()\n","invalid_hoa = game_team_df.filter(~col(\"HoA\").isin(\"home\", \"away\") | col(\"HoA\").isNull())\n","count_invalid_hoa = invalid_hoa.count()\n","sample_hoa = invalid_hoa.limit(5)\n","dropped_samples['Invalid HoA removed'] = sample_hoa\n","game_team_df = game_team_df.subtract(invalid_hoa)\n","count_after = game_team_df.count()\n","hoa_dropped = count_before - count_after\n","\n","# --- Step 7: Filter invalid settled_in ---\n","count_before = game_team_df.count()\n","invalid_settled_in = game_team_df.filter(~col(\"settled_in\").isin(\"reg\", \"ot\", \"so\") | col(\"settled_in\").isNull())\n","count_invalid_settled_in = invalid_settled_in.count()\n","sample_settled_in = invalid_settled_in.limit(5)\n","dropped_samples['Invalid settled_in removed'] = sample_settled_in\n","game_team_df = game_team_df.subtract(invalid_settled_in)\n","count_after = game_team_df.count()\n","settled_in_dropped = count_before - count_after\n","\n","# --- Step 8: Filter invalid startRinkSide ---\n","count_before = game_team_df.count()\n","invalid_startRinkSide = game_team_df.filter(~col(\"startRinkSide\").isin(\"left\", \"right\") | col(\"startRinkSide\").isNull())\n","count_invalid_startRinkSide = invalid_startRinkSide.count()\n","sample_startRinkSide = invalid_startRinkSide.limit(5)\n","dropped_samples['Invalid startRinkSide removed'] = sample_startRinkSide\n","game_team_df = game_team_df.subtract(invalid_startRinkSide)\n","count_after = game_team_df.count()\n","startRinkSide_dropped = count_before - count_after\n","\n","# --- Step 9a: Logical check powerPlayGoals > powerPlayOpportunities ---\n","count_before = game_team_df.count()\n","logical_ppg_gt_ppo = game_team_df.filter(col(\"powerPlayGoals\") > col(\"powerPlayOpportunities\"))\n","count_logical_ppg_gt_ppo = logical_ppg_gt_ppo.count()\n","sample_ppg_gt_ppo = logical_ppg_gt_ppo.limit(5)\n","dropped_samples['powerPlayGoals > powerPlayOpportunities removed'] = sample_ppg_gt_ppo\n","game_team_df = game_team_df.subtract(logical_ppg_gt_ppo)\n","count_after = game_team_df.count()\n","logical_ppg_gt_ppo_dropped = count_before - count_after\n","\n","# --- Step 9b: Logical check goals < powerPlayGoals ---\n","count_before = game_team_df.count()\n","logical_goals_lt_ppg = game_team_df.filter(col(\"goals\") < col(\"powerPlayGoals\"))\n","count_logical_goals_lt_ppg = logical_goals_lt_ppg.count()\n","sample_goals_lt_ppg = logical_goals_lt_ppg.limit(5)\n","dropped_samples['goals < powerPlayGoals removed'] = sample_goals_lt_ppg\n","game_team_df = game_team_df.subtract(logical_goals_lt_ppg)\n","count_after = game_team_df.count()\n","logical_goals_lt_ppg_dropped = count_before - count_after\n","\n","# --- Step 10: Check for multiple winners per game ---\n","multi_winners_df = game_team_df.filter(col(\"won\") == True) \\\n","    .groupBy(\"game_id\") \\\n","    .agg(count_(\"won\").alias(\"winner_count\")) \\\n","    .filter(col(\"winner_count\") > 1)\n","\n","multi_winners_list = [row[\"game_id\"] for row in multi_winners_df.collect()]\n","count_before = game_team_df.count()\n","multi_winners_rows = game_team_df.filter(col(\"game_id\").isin(multi_winners_list))\n","count_multi_winners_rows = multi_winners_rows.count()\n","sample_multi_winners = multi_winners_rows.limit(5)\n","dropped_samples['Multiple winners removed'] = sample_multi_winners\n","game_team_df = game_team_df.subtract(multi_winners_rows)\n","count_after = game_team_df.count()\n","multi_winners_dropped = count_before - count_after\n","\n","# Final cleaned count\n","final_count = game_team_df.count()\n","\n","# --- Schema changes summary ---\n","schema_changes_df = spark.createDataFrame(\n","    [(name, old_t, new_t) for name, old_t, new_t in schema_changes],\n","    schema=[\"Column\", \"Original Type\", \"New Type\"]\n",")\n","\n","print(\"🛠 Schema changes summary:\")\n","display(schema_changes_df)\n","\n","# --- Cleaning steps summary ---\n","summary_data = [\n","    (\"Initial row count (raw)\", initial_count, None),\n","    (\"After type casting\", count_after_type_cast, None),\n","    (\"Exact duplicates dropped\", exact_dupes_dropped, f\"{exact_dupes_dropped} rows dropped\"),\n","    (\"After dropping exact duplicates\", count_after_type_cast - exact_dupes_dropped, None),\n","    (\"Duplicate (game_id, team_id) dropped\", pair_dupes_dropped, f\"{pair_dupes_dropped} rows dropped\"),\n","    (\"After dropping (game_id, team_id) duplicates\", count_after_type_cast - exact_dupes_dropped - pair_dupes_dropped, None),\n","    (\"Negative values fixed\", count_after_neg_fix, \"Negative values replaced by zero (no rows dropped)\"),\n","    (\"Invalid faceOffWinPercentage removed\", faceoff_dropped, f\"{faceoff_dropped} rows dropped (null or outside 0-100)\"),\n","    (\"Invalid HoA removed\", hoa_dropped, f\"{hoa_dropped} rows dropped (invalid or null HoA)\"),\n","    (\"Invalid settled_in removed\", settled_in_dropped, f\"{settled_in_dropped} rows dropped (invalid or null settled_in)\"),\n","    (\"Invalid startRinkSide removed\", startRinkSide_dropped, f\"{startRinkSide_dropped} rows dropped (invalid or null startRinkSide)\"),\n","    (\"powerPlayGoals > powerPlayOpportunities removed\", logical_ppg_gt_ppo_dropped, f\"{logical_ppg_gt_ppo_dropped} rows dropped\"),\n","    (\"goals < powerPlayGoals removed\", logical_goals_lt_ppg_dropped, f\"{logical_goals_lt_ppg_dropped} rows dropped\"),\n","    (\"Multiple winners removed\", multi_winners_dropped, f\"{multi_winners_dropped} rows dropped\"),\n","    (\"Final cleaned row count\", final_count, None)\n","]\n","\n","summary_df = spark.createDataFrame(summary_data, [\"Step\", \"Row Count\", \"Details\"])\n","\n","print(\"📊 Cleaning Process Summary (step-by-step):\")\n","display(summary_df)\n","\n","# --- Show original data preview ---\n","print(\"🧐 Original raw data preview (50 rows):\")\n","display(raw_game_team_df.limit(5))\n","\n","# --- Show final cleaned data preview ---\n","print(\"✅ Final cleaned data preview (50 rows):\")\n","display(game_team_df.limit(5))\n","\n","# --- Show dropped examples per step ---\n","print(\"🔍 Examples of dropped rows at each cleaning step:\")\n","for step, sample_df in dropped_samples.items():\n","    print(f\"\\nStep: {step} — Showing up to 5 rows dropped\")\n","    display(sample_df)\n","\n","game_team_df.write.mode(\"overwrite\").saveAsTable(\"silver_game_team_stats\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3c82ea5b-56cc-4135-a641-3c11afedbaff"},{"cell_type":"markdown","source":["## game_shifts.csv\n","Loads raw game shift data, performs data type casting, and removes invalid or duplicate records. It filters out rows with nulls, negative times, or invalid period logic, then summarizes each cleaning step. Finally, it previews and saves the cleaned dataset to the Silver layer in the Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"32402254-da73-4f73-8278-4823e7fa55cd"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","from pyspark.sql.types import IntegerType, FloatType, StringType, StructType\n","\n","# Helper function to compare full original and new schema and show all columns\n","def full_schema_comparison_df(original_schema: StructType, new_schema: StructType):\n","    orig_types = {f.name: f.dataType.simpleString() for f in original_schema.fields}\n","    new_types = {f.name: f.dataType.simpleString() for f in new_schema.fields}\n","    all_columns = sorted(set(orig_types.keys()).union(set(new_types.keys())))\n","    rows = []\n","    for col_name in all_columns:\n","        orig_type = orig_types.get(col_name, \"N/A (added)\")\n","        new_type = new_types.get(col_name, \"N/A (removed)\")\n","        rows.append((col_name, orig_type, new_type))\n","    return spark.createDataFrame(rows, [\"Column\", \"Original Type\", \"New Type\"])\n","\n","# --- Step 0: Load raw data ---\n","raw_shift_df = spark.read.table(\"game_shifts\")\n","initial_count = raw_shift_df.count()\n","print(f\"Initial row count: {initial_count}\")\n","\n","# --- Show original schema ---\n","original_schema = raw_shift_df.schema\n","\n","# --- Step 1: Type casting ---\n","shift_df = raw_shift_df \\\n","    .withColumn(\"game_id\", col(\"game_id\").cast(StringType())) \\\n","    .withColumn(\"player_id\", col(\"player_id\").cast(StringType())) \\\n","    .withColumn(\"period\", col(\"period\").cast(IntegerType())) \\\n","    .withColumn(\"shift_start\", col(\"shift_start\").cast(FloatType())) \\\n","    .withColumn(\"shift_end\", col(\"shift_end\").cast(FloatType()))\n","\n","count_after_type_cast = shift_df.count()\n","\n","# --- Show full schema comparison ---\n","new_schema = shift_df.schema\n","schema_comparison_df = full_schema_comparison_df(original_schema, new_schema)\n","print(\"📋 Full Schema Comparison (all columns):\")\n","display(schema_comparison_df)\n","\n","# --- Step 2: Drop exact duplicates ---\n","count_before = count_after_type_cast\n","shift_df = shift_df.dropDuplicates()\n","count_after = shift_df.count()\n","exact_dupes_dropped = count_before - count_after\n","\n","# --- Step 3: Drop duplicates on shift keys ---\n","count_before = count_after\n","shift_df = shift_df.dropDuplicates([\"game_id\", \"player_id\", \"period\", \"shift_start\", \"shift_end\"])\n","count_after = shift_df.count()\n","pair_dupes_dropped = count_before - count_after\n","\n","# --- Step 4: Remove rows with nulls in mandatory columns ---\n","count_before = count_after\n","nulls_df = shift_df.filter(\n","    col(\"game_id\").isNull() |\n","    col(\"player_id\").isNull() |\n","    col(\"period\").isNull()\n",")\n","nulls_count = nulls_df.count()\n","\n","# Subtract only rows that fail mandatory checks\n","shift_df = shift_df.subtract(nulls_df)\n","count_after = shift_df.count()\n","nulls_dropped = count_before - count_after\n","\n","# --- Step 5: Filter invalid periods (period <= 0) ---\n","count_before = count_after\n","invalid_period_df = shift_df.filter(col(\"period\") <= 0)\n","invalid_period_count = invalid_period_df.count()\n","shift_df = shift_df.subtract(invalid_period_df)\n","count_after = shift_df.count()\n","period_dropped = count_before - count_after\n","\n","# --- Step 6: Filter invalid shift times (shift_start < 0 or shift_end < 0) ---\n","count_before = count_after\n","invalid_shift_time_df = shift_df.filter((col(\"shift_start\") < 0) | (col(\"shift_end\") < 0))\n","invalid_shift_time_count = invalid_shift_time_df.count()\n","shift_df = shift_df.subtract(invalid_shift_time_df)\n","count_after = shift_df.count()\n","shift_time_dropped = count_before - count_after\n","\n","# --- Step 7: Filter shifts where shift_end <= shift_start ---\n","count_before = count_after\n","invalid_shift_order_df = shift_df.filter(col(\"shift_end\") <= col(\"shift_start\"))\n","invalid_shift_order_count = invalid_shift_order_df.count()\n","shift_df = shift_df.subtract(invalid_shift_order_df)\n","count_after = shift_df.count()\n","shift_order_dropped = count_before - count_after\n","\n","\n","# --- Final cleaned count ---\n","final_count = shift_df.count()\n","\n","# --- Summary table ---\n","summary_data = [\n","    (\"Initial row count (raw)\", initial_count, None),\n","    (\"After type casting\", count_after_type_cast, None),\n","    (\"Exact duplicates dropped\", exact_dupes_dropped, f\"{exact_dupes_dropped} rows dropped\"),\n","    (\"Duplicate shift rows dropped\", pair_dupes_dropped, f\"{pair_dupes_dropped} rows dropped\"),\n","    (\"Rows with null mandatory fields dropped\", nulls_dropped, f\"{nulls_dropped} rows dropped\"),\n","    (\"Invalid periods (<=0) dropped\", period_dropped, f\"{period_dropped} rows dropped\"),\n","    (\"Invalid shift times (negative) dropped\", shift_time_dropped, f\"{shift_time_dropped} rows dropped\"),\n","    (\"Shifts with shift_end <= shift_start dropped\", shift_order_dropped, f\"{shift_order_dropped} rows dropped\"),\n","    (\"Final cleaned row count\", final_count, None)\n","]\n","\n","summary_df = spark.createDataFrame(summary_data, [\"Step\", \"Row Count\", \"Details\"])\n","\n","print(\"📊 Cleaning Process Summary (step-by-step):\")\n","display(summary_df)\n","\n","# --- Show samples of dropped rows for validation ---\n","print(\"🚨 Sample dropped rows for validation:\")\n","\n","def show_dropped_sample(name, df):\n","    print(f\"--- {name} ---\")\n","    if df.count() > 0:\n","        display(df.limit(5))\n","    else:\n","        print(\"No rows dropped here.\")\n","\n","# Note: Using subtract with original raw dataframe to find dropped rows only works for exact matches,\n","# so samples are shown from filtered dfs directly when available\n","show_dropped_sample(\"Exact duplicates\", raw_shift_df.subtract(shift_df))\n","show_dropped_sample(\"Duplicate shifts\", raw_shift_df.subtract(shift_df))\n","show_dropped_sample(\"Null mandatory fields\", nulls_df)\n","show_dropped_sample(\"Invalid periods\", invalid_period_df)\n","show_dropped_sample(\"Invalid shift times\", invalid_shift_time_df)\n","show_dropped_sample(\"Shifts with shift_end <= shift_start\", invalid_shift_order_df)\n","\n","# --- Preview cleaned data ---\n","print(\"✅ Cleaned Data Preview:\")\n","display(shift_df.limit(10))\n","\n","shift_df.write.mode(\"overwrite\").saveAsTable(\"silver_game_shifts\")\n","\n","print(\"✅ Cleaned data successfully saved to table: silver_game_shifts\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3445b844-8073-49ce-a1f2-9a03b2734480"},{"cell_type":"markdown","source":["## game_skater_stats.csv\n","Loads player game statistics from a CSV file and performs step-by-step data cleaning and validation. It casts column types, removes duplicates, handles missing or invalid values, and corrects logical inconsistencies (e.g., wins exceeding attempts or mismatched time-on-ice totals). Finally, it saves the cleaned dataset to the Silver layer in the Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"31f7751d-3ad3-4210-85ee-65deca361a4a"},{"cell_type":"code","source":["from pyspark.sql.functions import col, trim, lower, lit, udf\n","from pyspark.sql.types import IntegerType, FloatType, StringType\n","from functools import reduce\n","from operator import add\n","\n","# --- Helper function: Compare schema before & after ---\n","def full_schema_comparison_df(original_schema, new_schema):\n","    orig_types = {f.name: f.dataType.simpleString() for f in original_schema.fields}\n","    new_types = {f.name: f.dataType.simpleString() for f in new_schema.fields}\n","    all_columns = sorted(set(orig_types.keys()).union(set(new_types.keys())))\n","    rows = [(col_name, orig_types.get(col_name, \"N/A (added)\"), new_types.get(col_name, \"N/A (removed)\")) for col_name in all_columns]\n","    return spark.createDataFrame(rows, [\"Column\", \"Original Type\", \"New Type\"])\n","\n","# --- Step 0: Load raw data ---\n","raw_stats_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/game_skater_stats.csv\")\n","initial_count = raw_stats_df.count()\n","print(f\"Initial row count: {initial_count}\")\n","\n","# Save original schema\n","original_schema = raw_stats_df.schema\n","\n","# --- Step 1: Type Casting ---\n","stats_df = raw_stats_df \\\n","    .withColumn(\"game_id\", col(\"game_id\").cast(StringType())) \\\n","    .withColumn(\"player_id\", col(\"player_id\").cast(StringType())) \\\n","    .withColumn(\"team_id\", col(\"team_id\").cast(StringType())) \\\n","    .withColumn(\"timeOnIce\", col(\"timeOnIce\").cast(IntegerType())) \\\n","    .withColumn(\"assists\", col(\"assists\").cast(IntegerType())) \\\n","    .withColumn(\"goals\", col(\"goals\").cast(IntegerType())) \\\n","    .withColumn(\"shots\", col(\"shots\").cast(IntegerType())) \\\n","    .withColumn(\"hits\", col(\"hits\").cast(FloatType())) \\\n","    .withColumn(\"powerPlayGoals\", col(\"powerPlayGoals\").cast(IntegerType())) \\\n","    .withColumn(\"powerPlayAssists\", col(\"powerPlayAssists\").cast(IntegerType())) \\\n","    .withColumn(\"penaltyMinutes\", col(\"penaltyMinutes\").cast(IntegerType())) \\\n","    .withColumn(\"faceOffWins\", col(\"faceOffWins\").cast(IntegerType())) \\\n","    .withColumn(\"faceoffTaken\", col(\"faceoffTaken\").cast(IntegerType())) \\\n","    .withColumn(\"takeaways\", col(\"takeaways\").cast(FloatType())) \\\n","    .withColumn(\"giveaways\", col(\"giveaways\").cast(FloatType())) \\\n","    .withColumn(\"shortHandedGoals\", col(\"shortHandedGoals\").cast(IntegerType())) \\\n","    .withColumn(\"shortHandedAssists\", col(\"shortHandedAssists\").cast(IntegerType())) \\\n","    .withColumn(\"blocked\", col(\"blocked\").cast(IntegerType())) \\\n","    .withColumn(\"plusMinus\", col(\"plusMinus\").cast(IntegerType())) \\\n","    .withColumn(\"evenTimeOnIce\", col(\"evenTimeOnIce\").cast(IntegerType())) \\\n","    .withColumn(\"shortHandedTimeOnIce\", col(\"shortHandedTimeOnIce\").cast(IntegerType())) \\\n","    .withColumn(\"powerPlayTimeOnIce\", col(\"powerPlayTimeOnIce\").cast(IntegerType()))\n","\n","count_after_type_cast = stats_df.count()\n","\n","# --- Schema Comparison ---\n","new_schema = stats_df.schema\n","schema_comparison_df = full_schema_comparison_df(original_schema, new_schema)\n","print(\"📋 Full Schema Comparison (Original vs After Type Casting):\")\n","display(schema_comparison_df)\n","\n","# --- Dropped rows collector ---\n","dropped_samples = {}\n","\n","# --- Step 2: Drop exact duplicates ---\n","count_before = count_after_type_cast\n","exact_duplicates_df = stats_df.subtract(stats_df.dropDuplicates())\n","dropped_samples[\"Exact duplicates dropped\"] = exact_duplicates_df.limit(5)\n","stats_df = stats_df.dropDuplicates()\n","count_after = stats_df.count()\n","exact_dupes_dropped = count_before - count_after\n","\n","# --- Step 3: Drop duplicates on game_id, player_id, team_id ---\n","count_before = count_after\n","key_duplicates_df = stats_df.subtract(stats_df.dropDuplicates([\"game_id\", \"player_id\", \"team_id\"]))\n","dropped_samples[\"Duplicate keys dropped\"] = key_duplicates_df.limit(5)\n","stats_df = stats_df.dropDuplicates([\"game_id\", \"player_id\", \"team_id\"])\n","count_after = stats_df.count()\n","key_dupes_dropped = count_before - count_after\n","\n","# --- Step 4: Drop nulls in mandatory fields ---\n","mandatory_cols = [\"game_id\", \"player_id\", \"team_id\", \"timeOnIce\", \"assists\", \"goals\", \"shots\"]\n","count_before = count_after\n","null_condition = reduce(add, [col(c).isNull().cast(\"int\") for c in mandatory_cols]) > 0\n","nulls_df = stats_df.filter(null_condition)\n","dropped_samples[\"Rows with null mandatory fields dropped\"] = nulls_df.limit(5)\n","stats_df = stats_df.subtract(nulls_df)\n","count_after = stats_df.count()\n","nulls_dropped = count_before - count_after\n","\n","# --- Step 5: Remove rows with negative values (except plusMinus) ---\n","non_negative_cols = [\n","    \"timeOnIce\", \"assists\", \"goals\", \"shots\", \"hits\",\n","    \"powerPlayGoals\", \"powerPlayAssists\", \"penaltyMinutes\",\n","    \"faceOffWins\", \"faceoffTaken\", \"takeaways\", \"giveaways\",\n","    \"shortHandedGoals\", \"shortHandedAssists\", \"blocked\",\n","    \"evenTimeOnIce\", \"shortHandedTimeOnIce\", \"powerPlayTimeOnIce\"\n","]\n","\n","count_before = count_after\n","neg_condition = reduce(add, [(col(c) < 0).cast(\"int\") for c in non_negative_cols]) > 0\n","negatives_df = stats_df.filter(neg_condition)\n","dropped_samples[\"Negative numeric stats dropped\"] = negatives_df.limit(5)\n","stats_df = stats_df.subtract(negatives_df)\n","count_after = stats_df.count()\n","negatives_dropped = count_before - count_after\n","\n","# --- Step 6: faceOffWins > faceoffTaken ---\n","count_before = count_after\n","invalid_faceoff_df = stats_df.filter(col(\"faceOffWins\") > col(\"faceoffTaken\"))\n","dropped_samples[\"faceOffWins > faceoffTaken dropped\"] = invalid_faceoff_df.limit(5)\n","stats_df = stats_df.subtract(invalid_faceoff_df)\n","count_after = stats_df.count()\n","faceoff_dropped = count_before - count_after\n","\n","# --- Step 7: timeOnIce < sum of even/short/power play ---\n","count_before = count_after\n","inconsistent_time_df = stats_df.filter(\n","    col(\"timeOnIce\") < (col(\"evenTimeOnIce\") + col(\"shortHandedTimeOnIce\") + col(\"powerPlayTimeOnIce\"))\n",")\n","dropped_samples[\"timeOnIce inconsistency dropped\"] = inconsistent_time_df.limit(5)\n","stats_df = stats_df.subtract(inconsistent_time_df)\n","count_after = stats_df.count()\n","time_inconsistent_dropped = count_before - count_after\n","\n","# --- Step 8: Overwrite the table with full schema ---\n","# Optional: drop existing table\n","spark.sql(\"DROP TABLE IF EXISTS silver_game_skater_stats\")\n","\n","# Write the cleaned DataFrame as a new table\n","stats_df.write \\\n","    .mode(\"overwrite\") \\\n","    .option(\"overwriteSchema\", \"true\") \\\n","    .saveAsTable(\"silver_game_skater_stats\")\n","\n","print(\"✅ Table 'silver_game_skater_stats' created/overwritten successfully with full schema.\")\n","\n","# --- Final Count ---\n","final_count = stats_df.count()\n","\n","# --- Summary Table ---\n","summary_data = [\n","    (\"Initial row count (raw)\", initial_count, None),\n","    (\"After type casting\", count_after_type_cast, None),\n","    (\"Exact duplicates dropped\", exact_dupes_dropped, f\"{exact_dupes_dropped} rows dropped\"),\n","    (\"Duplicate keys dropped\", key_dupes_dropped, f\"{key_dupes_dropped} rows dropped\"),\n","    (\"Rows with null mandatory fields dropped\", nulls_dropped, f\"{nulls_dropped} rows dropped\"),\n","    (\"Negative numeric stats dropped\", negatives_dropped, f\"{negatives_dropped} rows dropped\"),\n","    (\"faceOffWins > faceoffTaken dropped\", faceoff_dropped, f\"{faceoff_dropped} rows dropped\"),\n","    (\"timeOnIce inconsistency dropped\", time_inconsistent_dropped, f\"{time_inconsistent_dropped} rows dropped\"),\n","    (\"Final cleaned row count\", final_count, None)\n","]\n","\n","summary_df = spark.createDataFrame(summary_data, [\"Step\", \"Row Count\", \"Details\"])\n","print(\"📊 Cleaning Process Summary (step-by-step):\")\n","display(summary_df)\n","\n","# --- Dropped rows preview ---\n","print(\"🚨 Sample dropped rows for verification:\")\n","for step, sample_df in dropped_samples.items():\n","    print(f\"\\n--- {step} ---\")\n","    display(sample_df)\n","\n","# --- Final Cleaned Data Preview ---\n","print(\"✅ Cleaned Data Preview:\")\n","display(stats_df.limit(10))\n","\n","\n","stats_df.write.mode(\"overwrite\").saveAsTable(\"silver_game_skater_stats\")\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d3bd4a02-29ee-4161-a1d0-4730aa76a1bf"},{"cell_type":"markdown","source":["## player_info.csv\n","Loads player information from a CSV file and performs extensive cleaning and validation. Key operations include type casting, handling nulls, removing duplicates, converting height and weight units, and validating height, weight, primary position, and shoots/catches fields. Finally, the cleaned dataset is saved to the Silver layer in the Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"feb11a20-3414-4ad5-945c-68c1f9595ff8"},{"cell_type":"code","source":["from pyspark.sql.functions import col, lower, trim, when, lit, udf\n","from pyspark.sql.types import IntegerType, FloatType, StringType, DateType, DoubleType, StructType\n","from functools import reduce\n","from operator import add\n","\n","# --- Helper: Full schema comparison ---\n","def full_schema_comparison_df(original_schema: StructType, new_schema: StructType):\n","    orig_types = {f.name: f.dataType.simpleString() for f in original_schema.fields}\n","    new_types = {f.name: f.dataType.simpleString() for f in new_schema.fields}\n","    all_columns = sorted(set(orig_types.keys()).union(set(new_types.keys())))\n","    rows = [(col_name, orig_types.get(col_name, \"N/A (added)\"), new_types.get(col_name, \"N/A (removed)\")) for col_name in all_columns]\n","    return spark.createDataFrame(rows, [\"Column\", \"Original Type\", \"New Type\"])\n","\n","# --- Step 0: Load raw data ---\n","raw_player_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/player_info.csv\")\n","initial_count = raw_player_df.count()\n","print(f\"Initial row count: {initial_count}\")\n","\n","\n","# Save original schema\n","original_schema = raw_player_df.schema\n","\n","# --- Step 1: Type casting & cleaning ---\n","\n","# UDF to convert height string (e.g., \"6' 2\\\"\") to cm\n","def height_to_cm(height_str):\n","    import re\n","    if height_str is None:\n","        return None\n","    match = re.match(r\"(\\d+)' (\\d+)\\\"\", height_str)\n","    if match:\n","        feet = int(match.group(1))\n","        inches = int(match.group(2))\n","        return round(feet * 30.48 + inches * 2.54, 2)\n","    return None\n","\n","height_to_cm_udf = udf(height_to_cm, DoubleType())\n","\n","player_df = raw_player_df \\\n","    .withColumn(\"nationality\", lower(trim(col(\"nationality\")))) \\\n","    .withColumn(\"birthCity\", trim(col(\"birthCity\"))) \\\n","    .withColumn(\"primaryPosition\", lower(trim(col(\"primaryPosition\")))) \\\n","    .withColumn(\"birthDate\", col(\"birthDate\").cast(DateType())) \\\n","    .withColumn(\"birthStateProvince\", lower(trim(col(\"birthStateProvince\")))) \\\n","    .withColumn(\"height_cm\", when(col(\"height_cm\").isNull(), height_to_cm_udf(col(\"height\"))).otherwise(col(\"height_cm\"))) \\\n","    .withColumn(\"height_cm\", col(\"height_cm\").cast(FloatType())) \\\n","    .withColumn(\"weight\", col(\"weight\").cast(FloatType())) \\\n","    .withColumn(\"shootsCatches\", lower(trim(col(\"shootsCatches\")))) \\\n","    .withColumn(\"player_id\", col(\"player_id\").cast(StringType()))\n","\n","# --- New Step: Convert weight lbs to kg in new column weight_kg ---\n","player_df = player_df.withColumn(\"weight_kg\", (col(\"weight\") * 0.453592).cast(FloatType()))\n","\n","count_after_type_cast = player_df.count()\n","\n","# Show full schema comparison\n","new_schema = player_df.schema\n","schema_comparison_df = full_schema_comparison_df(original_schema, new_schema)\n","print(\"📋 Full Schema Comparison (Original vs After Type Casting):\")\n","display(schema_comparison_df)\n","\n","# --- Step 2+: Cleaning steps ---\n","dropped_samples = {}\n","\n","# Step 2: Drop exact duplicates\n","count_before = count_after_type_cast\n","exact_duplicates_df = player_df.subtract(player_df.dropDuplicates())\n","dropped_samples['Exact duplicates dropped'] = exact_duplicates_df.limit(5)\n","player_df = player_df.dropDuplicates()\n","count_after = player_df.count()\n","exact_dupes_dropped = count_before - count_after\n","\n","# Step 3: Drop duplicates on player_id\n","if \"player_id\" in player_df.columns:\n","    count_before = count_after\n","    player_id_dupes_df = player_df.subtract(player_df.dropDuplicates([\"player_id\"]))\n","    dropped_samples['Duplicate player_id dropped'] = player_id_dupes_df.limit(5)\n","    player_df = player_df.dropDuplicates([\"player_id\"])\n","    count_after = player_df.count()\n","    player_id_dupes_dropped = count_before - count_after\n","else:\n","    player_id_dupes_dropped = 0\n","\n","# Step 4: Drop rows with nulls in mandatory fields\n","mandatory_cols = [\"player_id\", \"nationality\", \"primaryPosition\", \"birthDate\", \"height_cm\", \"weight_kg\", \"shootsCatches\"]\n","count_before = count_after\n","null_condition = reduce(add, [col(c).isNull().cast(\"int\") for c in mandatory_cols]) > 0\n","nulls_df = player_df.filter(null_condition)\n","dropped_samples['Rows with null mandatory fields dropped'] = nulls_df.limit(5)\n","player_df = player_df.subtract(nulls_df)\n","count_after = player_df.count()\n","nulls_dropped = count_before - count_after\n","\n","# Step 5: Validate height_cm (140–250 cm)\n","count_before = count_after\n","invalid_height_df = player_df.filter((col(\"height_cm\") < 140) | (col(\"height_cm\") > 250) | col(\"height_cm\").isNull())\n","dropped_samples['Invalid height_cm removed'] = invalid_height_df.limit(5)\n","player_df = player_df.subtract(invalid_height_df)\n","count_after = player_df.count()\n","height_dropped = count_before - count_after\n","\n","# Step 6: Validate weight_kg (40–200 kg)\n","count_before = count_after\n","invalid_weight_df = player_df.filter((col(\"weight_kg\") < 40) | (col(\"weight_kg\") > 200) | col(\"weight_kg\").isNull())\n","dropped_samples['Invalid weight removed'] = invalid_weight_df.limit(5)\n","player_df = player_df.subtract(invalid_weight_df)\n","count_after = player_df.count()\n","weight_dropped = count_before - count_after\n","\n","# Step 7: Validate primaryPosition\n","valid_positions = [\"c\", \"lw\", \"rw\", \"d\", \"g\"]\n","count_before = count_after\n","invalid_pos_df = player_df.filter(~col(\"primaryPosition\").isin(valid_positions) | col(\"primaryPosition\").isNull())\n","dropped_samples['Invalid primaryPosition removed'] = invalid_pos_df.limit(5)\n","player_df = player_df.subtract(invalid_pos_df)\n","count_after = player_df.count()\n","pos_dropped = count_before - count_after\n","\n","# Step 8: Validate shootsCatches\n","count_before = count_after\n","invalid_shoots_df = player_df.filter(~col(\"shootsCatches\").isin(\"l\", \"r\") | col(\"shootsCatches\").isNull())\n","dropped_samples['Invalid shootsCatches removed'] = invalid_shoots_df.limit(5)\n","player_df = player_df.subtract(invalid_shoots_df)\n","count_after = player_df.count()\n","shoots_dropped = count_before - count_after\n","\n","# --- Final count ---\n","final_count = player_df.count()\n","\n","# --- Summary ---\n","summary_data = [\n","    (\"Initial row count (raw)\", initial_count, None),\n","    (\"After type casting\", count_after_type_cast, None),\n","    (\"Exact duplicates dropped\", exact_dupes_dropped, f\"{exact_dupes_dropped} rows dropped\"),\n","    (\"Duplicate player_id dropped\", player_id_dupes_dropped, f\"{player_id_dupes_dropped} rows dropped\"),\n","    (\"Rows with null mandatory fields dropped\", nulls_dropped, f\"{nulls_dropped} rows dropped\"),\n","    (\"Invalid height_cm removed\", height_dropped, f\"{height_dropped} rows dropped\"),\n","    (\"Invalid weight removed\", weight_dropped, f\"{weight_dropped} rows dropped\"),\n","    (\"Invalid primaryPosition removed\", pos_dropped, f\"{pos_dropped} rows dropped\"),\n","    (\"Invalid shootsCatches removed\", shoots_dropped, f\"{shoots_dropped} rows dropped\"),\n","    (\"Final cleaned row count\", final_count, None)\n","]\n","\n","summary_df = spark.createDataFrame(summary_data, [\"Step\", \"Row Count\", \"Details\"])\n","print(\"📊 Cleaning Process Summary (step-by-step):\")\n","display(summary_df)\n","\n","# --- Preview raw vs cleaned data ---\n","print(\"🧐 Original raw data preview (5 rows):\")\n","display(raw_player_df.limit(5))\n","\n","print(\"✅ Final cleaned data preview (5 rows):\")\n","display(player_df.limit(5))\n","\n","# --- Dropped row samples ---\n","print(\"🔍 Examples of dropped rows at each cleaning step:\")\n","for step, sample_df in dropped_samples.items():\n","    print(f\"\\nStep: {step} — Showing up to 5 rows dropped\")\n","    display(sample_df)\n","\n","\n","player_df.write.mode(\"overwrite\").saveAsTable(\"silver_player_info\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e14d2ff3-6fdb-4469-9d57-95592713c122"},{"cell_type":"markdown","source":["## game_goalie_stats.csv\n","Reads the game_goalie_stats.csv file and casts each column to the appropriate data type. It cleans the data by removing exact duplicates, checking for duplicate game_id and player_id combinations, filling missing values, and handling negative values. Finally, it saves the cleaned DataFrame to the Lakehouse Silver layer table."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"371afdff-8f64-4ac4-94eb-1559fe8ebba6"},{"cell_type":"code","source":["from pyspark.sql.functions import col, count, when, isnull, sum as pyspark_sum\n","from pyspark.sql.types import StringType, IntegerType, FloatType\n","\n","# Read csv from current directory\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/game_goalie_stats.csv\")\n","\n","# Verify data loaded successfully\n","print(f\"Row count: {df.count()}\")\n","print(\"Column names:\")\n","print(df.columns)\n","print(\"\\nSchema:\")\n","df.printSchema()\n","\n","print(\"\\nFirst 5 rows:\")\n","df.show(5, truncate=False)\n","\n","# Basic Data Cleaning Steps\n","if df is not None:\n","    print(\"\\n=== Starting Data Cleaning ===\")\n","    \n","    # 1. Fix data types based on data dictionary\n","    df_cleaned = df.withColumn(\"game_id\", col(\"game_id\").cast(StringType())) \\\n","                   .withColumn(\"player_id\", col(\"player_id\").cast(StringType())) \\\n","                   .withColumn(\"team_id\", col(\"team_id\").cast(StringType())) \\\n","                   .withColumn(\"timeOnIce\", col(\"timeOnIce\").cast(IntegerType())) \\\n","                   .withColumn(\"assists\", col(\"assists\").cast(IntegerType())) \\\n","                   .withColumn(\"goals\", col(\"goals\").cast(IntegerType())) \\\n","                   .withColumn(\"pim\", col(\"pim\").cast(IntegerType())) \\\n","                   .withColumn(\"shots\", col(\"shots\").cast(IntegerType())) \\\n","                   .withColumn(\"saves\", col(\"saves\").cast(IntegerType())) \\\n","                   .withColumn(\"powerPlaySaves\", col(\"powerPlaySaves\").cast(IntegerType())) \\\n","                   .withColumn(\"shortHandedSaves\", col(\"shortHandedSaves\").cast(IntegerType())) \\\n","                   .withColumn(\"evenSaves\", col(\"evenSaves\").cast(IntegerType())) \\\n","                   .withColumn(\"shortHandedShotsAgainst\", col(\"shortHandedShotsAgainst\").cast(IntegerType())) \\\n","                   .withColumn(\"evenShotsAgainst\", col(\"evenShotsAgainst\").cast(IntegerType())) \\\n","                   .withColumn(\"powerPlayShotsAgainst\", col(\"powerPlayShotsAgainst\").cast(IntegerType())) \\\n","                   .withColumn(\"decision\", col(\"decision\").cast(StringType())) \\\n","                   .withColumn(\"savePercentage\", col(\"savePercentage\").cast(FloatType())) \\\n","                   .withColumn(\"powerPlaySavePercentage\", col(\"powerPlaySavePercentage\").cast(FloatType())) \\\n","                   .withColumn(\"evenStrengthSavePercentage\", col(\"evenStrengthSavePercentage\").cast(FloatType()))\n","    print(\"Schema after type casting:\")\n","    df_cleaned.printSchema()\n","\n","    # 2.1: Check for exact duplicates (all columns identical)\n","    print(\"\\n=== Checking for Exact Duplicates ===\")\n","    # Count occurrences of each row\n","    duplicate_counts = df_cleaned.groupBy(df_cleaned.columns).count().filter(col(\"count\") > 1)\n","    duplicate_count = duplicate_counts.count()\n","    if duplicate_count > 0:\n","        print(f\"Found {duplicate_count} groups of exact duplicate rows\")\n","        print(\"Sample of exact duplicate rows:\")\n","        duplicate_counts.show(5, truncate=False)\n","    else:\n","        print(\"No exact duplicate rows found\")\n","    # Decision: Drop exact duplicates (exact replicas likely errors)\n","    if duplicate_count > 0:\n","        print(\"Dropping exact duplicate rows...\")\n","        df_cleaned = df_cleaned.dropDuplicates()\n","        print(f\"Rows after dropping exact duplicates: {df_cleaned.count()}\")\n","\n","    # 2.2: Check for game_id and player_id group duplicates (non-exact duplicates)\n","    print(\"\\nChecking for duplicates in game_id and player_id groups...\")\n","    game_player_duplicates = df_cleaned.groupBy(\"game_id\", \"player_id\").count().filter(col(\"count\") > 1)\n","    game_player_duplicate_count = game_player_duplicates.count()\n","    if game_player_duplicate_count > 0:\n","        print(f\"Found {game_player_duplicate_count} groups of game_id and player_id duplicates\")\n","        print(\"Sample of game_id and player_id duplicates (top 5 groups):\")\n","        game_player_duplicates.show(5, truncate=False)\n","    else:\n","        print(\"No game_id and player_id duplicates found\")\n","    # Decision: Don't drop now\n","    # If game_player had duplicate, don't drop it now\n","\n","    # 3: Handle missing values\n","    print(\"\\n=== Handling Missing Values ===\")\n","    # Show missing value summary before cleaning\n","    print(\"Missing value summary before cleaning:\")\n","    missing_summary_before = df_cleaned.select([count(when(col(c).isNull(), c)).alias(c) for c in df_cleaned.columns])\n","    missing_summary_before.show(truncate=False)\n","\n","    # Handle decision column\n","    # Replace nulls with \"ND\" (No Decision)\n","    df_cleaned = df_cleaned.withColumn(\n","        \"decision\",\n","        when(col(\"decision\").isNull(), \"ND\").otherwise(col(\"decision\"))\n","    )\n","    \n","    # Handle powerPlaySavePercentage column\n","    # Replace nulls with 0.0 (no power play shots faced)\n","    df_cleaned = df_cleaned.withColumn(\n","        \"powerPlaySavePercentage\",\n","        when(col(\"powerPlaySavePercentage\").isNull(), 0.0).otherwise(col(\"powerPlaySavePercentage\"))\n","    )\n","\n","    # Show missing value summary after cleaning\n","    print(\"Missing value summary after cleaning:\")\n","    missing_summary_after = df_cleaned.select([count(when(col(c).isNull(), c)).alias(c) for c in df_cleaned.columns])\n","    missing_summary_after.show(truncate=False)\n","\n","    # 4. Handle negative values in shortHandedSaves\n","    df_cleaned = df_cleaned.withColumn(\n","        \"shortHandedSaves\",\n","        when(col(\"shortHandedSaves\") < 0, 0).otherwise(col(\"shortHandedSaves\"))\n","    )\n","\n","    # 5. Handle high percentage of zeros in certain columns (optional: leave as is for now)\n","    # Columns like pim (96.5% zeros), shortHandedSaves (52.4% zeros), etc., are valid as zeros are meaningful in context\n","\n","    # Print cleaned DataFrame info\n","    print(f\"\\nCleaned DataFrame:\")\n","    print(f\"Row count: {df_cleaned.count()}\")\n","    print(\"\\nCleaned sample data:\")\n","    df_cleaned.show(5, truncate=False)\n","\n","    # Check for null values after cleaning\n","    print(\"\\nNull value check after cleaning:\")\n","    null_counts = df_cleaned.select([pyspark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_cleaned.columns])\n","    null_counts.show()\n","\n","    # Save cleaned DataFrame to Lakehouse (Silver layer)\n","    (\n","        df_cleaned.write\n","        .mode(\"overwrite\")\n","        .saveAsTable(\"silver_goalie_stats\")\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"716a226e-99df-45b3-8979-13dda3551291"},{"cell_type":"markdown","source":["## game_goals.csv\n","Reads the game_goals.csv file and casts columns to appropriate types, filling missing Boolean values with False. It cleans the data by removing exact duplicates and standardizing the strength column to lowercase. Finally, the cleaned DataFrame is saved to the Lakehouse Silver layer table."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"651ead86-9b10-49cc-855d-49157356ae30"},{"cell_type":"code","source":["from pyspark.sql.functions import col, when, isnull, sum as pyspark_sum, lower\n","from pyspark.sql.types import StringType, BooleanType\n","\n","# Read csv from current directory\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/game_goals.csv\")\n","\n","# Verify data loaded successfully\n","print(f\"\\nOriginal DataFrame loaded successfully!\")\n","print(f\"Row count: {df.count()}\")\n","print(\"Column names:\")\n","print(df.columns)\n","print(\"\\nSchema:\")\n","df.printSchema()\n","\n","print(\"\\nFirst 5 rows:\")\n","df.show(5, truncate=False)\n","\n","# Basic Data Cleaning Steps\n","if df is not None:\n","    print(\"\\n=== Starting Data Cleaning ===\")\n","    \n","    # 1. Cast Boolean columns to correct type first to avoid type mismatch\n","    df_cleaned = df.withColumn(\n","        \"gameWinningGoal\",\n","        when(col(\"gameWinningGoal\").cast(BooleanType()).isNull(), False).otherwise(col(\"gameWinningGoal\").cast(BooleanType()))\n","    ).withColumn(\n","        \"emptyNet\",\n","        when(col(\"emptyNet\").cast(BooleanType()).isNull(), False).otherwise(col(\"emptyNet\").cast(BooleanType()))\n","    )\n","    \n","    # 2. Fix data types (ensure correct types for all columns)\n","    df_cleaned = df_cleaned.withColumn(\"play_id\", col(\"play_id\").cast(StringType())) \\\n","                           .withColumn(\"strength\", col(\"strength\").cast(StringType())) \\\n","                           .withColumn(\"gameWinningGoal\", col(\"gameWinningGoal\").cast(BooleanType())) \\\n","                           .withColumn(\"emptyNet\", col(\"emptyNet\").cast(BooleanType()))\n","    print(\"Schema after type casting:\")\n","    df_cleaned.printSchema()\n","\n","    # 3. Check for exact duplicates (all columns identical)\n","    print(\"\\n=== Checking for Exact Duplicates ===\")\n","    # Count occurrences of each row\n","    duplicate_counts = df_cleaned.groupBy(df_cleaned.columns).count().filter(col(\"count\") > 1)\n","    duplicate_count = duplicate_counts.count()\n","    if duplicate_count > 0:\n","        print(f\"Found {duplicate_count} groups of exact duplicate rows\")\n","        print(\"Sample of exact duplicate rows:\")\n","        duplicate_counts.show(5, truncate=False)\n","    else:\n","        print(\"No exact duplicate rows found\")\n","    # Decision: Drop exact duplicates (exact replicas likely errors)\n","    original_count = df_cleaned.count()\n","    df_cleaned = df_cleaned.dropDuplicates()\n","    print(f\"Removed {original_count - df_cleaned.count()} exact duplicate rows\")\n","    \n","    # 4. Handle imbalanced emptyNet column (optional: filter out rare True values if needed)\n","    # For now, we'll keep all values but verify distribution\n","    print(\"\\nDistribution of emptyNet values:\")\n","    df_cleaned.groupBy(\"emptyNet\").count().show()\n","    \n","    # 5. Standardize strength column (convert to lowercase)\n","    df_cleaned = df_cleaned.withColumn(\n","        \"strength\",\n","        when(col(\"strength\").isNotNull(), lower(col(\"strength\"))).otherwise(col(\"strength\"))\n","    )\n","    \n","    # Print cleaned DataFrame info\n","    print(f\"\\nCleaned DataFrame:\")\n","    print(f\"Row count: {df_cleaned.count()}\")\n","    print(\"\\nCleaned sample data:\")\n","    df_cleaned.show(5, truncate=False)\n","    \n","    # Check for null values after cleaning\n","    print(\"\\nNull value check after cleaning:\")\n","    null_counts = df_cleaned.select([pyspark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_cleaned.columns])\n","    null_counts.show()\n","    \n","    # Save cleaned DataFrame to Lakehouse (Silver layer)\n","    (\n","        df_cleaned.write\n","        .mode(\"overwrite\")\n","        .saveAsTable(\"silver_goals\")\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"34b22eae-da25-4730-8024-36e4cec3addb"},{"cell_type":"markdown","source":["## game_officials.csv\n","Reads the game_officials.csv file, casts columns to appropriate types, and standardizes text (e.g., title-casing official names and normalizing official_type to either Referee or Linesman). It cleans the data by removing exact duplicates and filtering out invalid game_id or official_type values. Finally, the cleaned DataFrame is saved to the Lakehouse Silver layer table."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"da99979f-b3c5-49de-afde-a17e6ad71b1a"},{"cell_type":"code","source":["from pyspark.sql.functions import col, when, trim, lower, initcap, sum as pyspark_sum, count\n","from pyspark.sql.types import IntegerType, StringType\n","\n","# Read csv from current directory\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/game_officials.csv\")\n","\n","# Verify we successfully read the data\n","print(f\"\\nOriginal DataFrame loaded successfully!\")\n","print(f\"Row count: {df.count()}\")\n","print(\"Column names:\")\n","print(df.columns)\n","print(\"\\nSchema:\")\n","df.printSchema()\n","\n","print(\"\\nFirst 5 rows:\")\n","df.show(5, truncate=False)\n","\n","# Basic Data Cleaning Steps (only if data was loaded successfully)\n","if df is not None:\n","    print(\"\\n=== Starting Data Cleaning ===\")\n","    \n","    # 1. Ensure correct data types\n","    df_cleaned = df.withColumn(\"game_id\", col(\"game_id\").cast(StringType())) \\\n","                   .withColumn(\"official_name\", col(\"official_name\").cast(StringType())) \\\n","                   .withColumn(\"official_type\", col(\"official_type\").cast(StringType()))\n","    \n","    # 2. Standardize text columns\n","    # - Trim whitespace and convert official_name to title case\n","    # - Ensure official_type is either 'Referee' or 'Linesman'\n","    df_cleaned = df_cleaned.withColumn(\"official_name\", trim(col(\"official_name\"))) \\\n","                          .withColumn(\"official_name\", \n","                                      when(col(\"official_name\").isNotNull(),\n","                                           initcap(lower(col(\"official_name\")))).otherwise(col(\"official_name\"))) \\\n","                          .withColumn(\"official_type\",\n","                                      when(lower(col(\"official_type\")).isin(\"referee\", \"linesman\"),\n","                                           initcap(lower(col(\"official_type\")))).otherwise(\"Unknown\"))\n","    print(\"Schema after type casting:\")\n","    df_cleaned.printSchema()\n","    \n","    # 3: Check for exact duplicates (all columns identical)\n","    print(\"\\n=== Checking for Exact Duplicates ===\")\n","    # Count occurrences of each row\n","    duplicate_counts = df_cleaned.groupBy([\"game_id\", \"official_name\", \"official_type\"]).agg(count(\"*\").alias(\"row_count\")).filter(col(\"row_count\") > 1)\n","    if duplicate_count > 0:\n","        print(f\"Found {duplicate_count} groups of exact duplicate rows\")\n","        print(\"Sample of exact duplicate rows:\")\n","        duplicate_counts.show(5, truncate=False)\n","    else:\n","        print(\"No exact duplicate rows found\")\n","    \n","    # Decision: Drop exact duplicates (exact replicas likely errors)\n","    original_count = df_cleaned.count()\n","    df_cleaned = df_cleaned.dropDuplicates()\n","    print(f\"Removed {original_count - df_cleaned.count()} exact duplicate rows\")\n","    \n","    # 4. Validate game_id (ensure no negative values, as per profile report)\n","    if \"game_id\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.filter(col(\"game_id\") > 0)\n","        print(f\"Filtered out any rows with invalid (negative) game_id\")\n","    \n","    # 5. Validate official_type (ensure only 'Referee' or 'Linesman')\n","    df_cleaned = df_cleaned.filter(col(\"official_type\").isin(\"Referee\", \"Linesman\"))\n","    print(f\"Filtered rows to ensure official_type is either 'Referee' or 'Linesman'\")\n","    \n","    # Print cleaned DataFrame info\n","    print(f\"\\nCleaned DataFrame:\")\n","    print(f\"Row count: {df_cleaned.count()}\")\n","    print(\"\\nCleaned sample data:\")\n","    df_cleaned.show(5, truncate=False)\n","    \n","    # Check for null values\n","    print(\"\\nNull value check after cleaning:\")\n","    null_counts = df_cleaned.select([pyspark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_cleaned.columns])\n","    null_counts.show()\n","    \n","    # Save cleaned DataFrame to Lakehouse (Silver layer)\n","    (\n","        df_cleaned.write\n","        .mode(\"overwrite\")\n","        .saveAsTable(\"silver_officials\")\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"04bf4153-6b79-40bc-8ca5-7aee45ca30ff"},{"cell_type":"markdown","source":["## game_penalties.csv\n","Reads the game_penalties.csv file, casts columns to appropriate types, and handles missing values (e.g., filling null penaltySeverity with \"Unknown\"). It cleans the data by removing exact duplicates, validating play_id format, and logging distributions of key columns. Finally, the cleaned DataFrame is saved to the Lakehouse Silver layer table."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f4fd3c5-11c2-4657-8ddf-ebfeab1c10ff"},{"cell_type":"code","source":["from pyspark.sql.functions import col, when, isnan, isnull, sum as pyspark_sum\n","from pyspark.sql.types import StringType, IntegerType\n","\n","# Read csv from current directory\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/game_penalties.csv\")\n","\n","# Verify we successfully read the data\n","print(f\"\\nOriginal DataFrame loaded successfully!\")\n","print(f\"Row count: {df.count()}\")\n","print(\"Column names:\")\n","print(df.columns)\n","print(\"\\nSchema:\")\n","df.printSchema()\n","\n","print(\"\\nFirst 5 rows:\")\n","df.show(5, truncate=False)\n","\n","# Basic Data Cleaning Steps (only if data was loaded successfully)\n","df_cleaned = df.withColumn(\n","        \"penaltySeverity\",\n","        when(col(\"penaltySeverity\").isNull() | isnan(col(\"penaltySeverity\")), \"Unknown\").otherwise(col(\"penaltySeverity\"))\n","    )\n","\n","if df is not None:\n","    print(\"\\n=== Starting Data Cleaning ===\")\n","    \n","    # 1. Fix data types\n","    if \"penaltyMinutes\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.withColumn(\"penaltyMinutes\", col(\"penaltyMinutes\").cast(IntegerType()))\n","    if \"penaltySeverity\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.withColumn(\"penaltySeverity\", col(\"penaltySeverity\").cast(StringType()))\n","    if \"play_id\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.withColumn(\"play_id\", col(\"play_id\").cast(StringType()))\n","    print(\"Schema after type casting:\")\n","    df_cleaned.printSchema()\n","\n","    # 2. Check for exact duplicates (all columns identical)\n","    print(\"\\n=== Checking for Exact Duplicates ===\")\n","    # Count occurrences of each row\n","    duplicate_counts = df_cleaned.groupBy(df_cleaned.columns).count().filter(col(\"count\") > 1)\n","    duplicate_count = duplicate_counts.count()\n","    if duplicate_count > 0:\n","        print(f\"Found {duplicate_count} groups of exact duplicate rows\")\n","        print(\"Sample of exact duplicate rows:\")\n","        duplicate_counts.show(5, truncate=False)\n","    else:\n","        print(\"No exact duplicate rows found\")\n","    # Decision: Drop exact duplicates (exact replicas likely errors)\n","    original_count = df_cleaned.count()\n","    df_cleaned = df_cleaned.dropDuplicates()\n","    print(f\"Removed {original_count - df_cleaned.count()} exact duplicate rows\")\n","    \n","    # 3. Address imbalance in penaltySeverity and penaltyMinutes\n","    # For simplicity, we'll just log the counts for now\n","    if \"penaltySeverity\" in df_cleaned.columns:\n","        print(\"\\nPenalty Severity distribution:\")\n","        df_cleaned.groupBy(\"penaltySeverity\").count().show()\n","    \n","    if \"penaltyMinutes\" in df_cleaned.columns:\n","        print(\"\\nPenalty Minutes distribution:\")\n","        df_cleaned.groupBy(\"penaltyMinutes\").count().show()\n","    \n","    # 4. Validate play_id format (should be 12-14 characters)\n","    if \"play_id\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.withColumn(\n","            \"play_id_valid\",\n","            when(col(\"play_id\").cast(StringType()).rlike(\"^[0-9]{10}_[0-9]+$\"), True).otherwise(False)\n","        )\n","        invalid_play_ids = df_cleaned.filter(~col(\"play_id_valid\")).count()\n","        print(f\"Found {invalid_play_ids} rows with invalid play_id format\")\n","        df_cleaned = df_cleaned.drop(\"play_id_valid\")  # Drop temporary column\n","    \n","    # Print cleaned DataFrame info\n","    print(f\"\\nCleaned DataFrame:\")\n","    print(f\"Row count: {df_cleaned.count()}\")\n","    print(\"\\nCleaned sample data:\")\n","    df_cleaned.show(5, truncate=False)\n","    \n","    # Check for null values after cleaning\n","    print(\"\\nNull value check after cleaning:\")\n","    null_counts = df_cleaned.select([pyspark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_cleaned.columns])\n","    null_counts.show()\n","    \n","    # Save cleaned DataFrame to Lakehouse (Silver layer)\n","    (\n","        df_cleaned.write\n","        .mode(\"overwrite\")\n","        .saveAsTable(\"silver_penalties\")\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08c5799a-29d7-4cbb-8f66-ae8dc0d00ac2"},{"cell_type":"markdown","source":["## game_plays.csv\n","Reads the game_plays.csv file, fills missing values (e.g., coordinates with 0, team_id with -1, secondaryType with \"Unknown\"), and casts columns to appropriate types. It cleans the data by removing exact duplicates, deduplicating play_id, and validating play_id format while logging distributions and invalid entries. Finally, the cleaned DataFrame is saved to the Lakehouse Silver layer table."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"55b96a47-72c6-445d-b807-c0fd7e694ba9"},{"cell_type":"code","source":["from pyspark.sql.functions import col, when, isnull, sum as pyspark_sum\n","from pyspark.sql.types import StringType, IntegerType, FloatType, TimestampType\n","\n","# Read csv from current directory\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze(raw datasets)/game_plays.csv\")\n","\n","# Verify we successfully read the data\n","print(f\"\\nOriginal DataFrame loaded successfully!\")\n","print(f\"Row count: {df.count()}\")\n","print(\"Column names:\")\n","print(df.columns)\n","print(\"\\nSchema:\")\n","df.printSchema()\n","\n","print(\"\\nFirst 5 rows:\")\n","df.show(5, truncate=False)\n","\n","# Basic Data Cleaning Steps (only if data was loaded successfully)\n","if df is not None:\n","    print(\"\\n=== Starting Data Cleaning ===\")\n","    \n","    # 1. Handle missing value\n","    # 1.1: For secondaryType - fill with \"Unknown\"\n","    df_cleaned = df.withColumn(\n","        \"secondaryType\",\n","        when(isnull(col(\"secondaryType\")), \"Unknown\").otherwise(col(\"secondaryType\"))\n","    )\n","    \n","    # 1.2: For coordinates (x, y, st_x, st_y) - fill with 0 (assuming rink center or neutral)\n","    coord_cols = [\"x\", \"y\", \"st_x\", \"st_y\"]\n","    for col_name in coord_cols:\n","        if col_name in df_cleaned.columns:\n","            df_cleaned = df_cleaned.withColumn(\n","                col_name,\n","                when(isnull(col(col_name)), 0).otherwise(col(col_name))\n","            )\n","    \n","    # 1.3: For team_id_for and team_id_against - fill with -1 (unknown team)\n","    team_cols = [\"team_id_for\", \"team_id_against\"]\n","    for col_name in team_cols:\n","        if col_name in df_cleaned.columns:\n","            df_cleaned = df_cleaned.withColumn(\n","                col_name,\n","                when(isnull(col(col_name)), -1).otherwise(col(col_name))\n","            )\n","    \n","    # 1.4: For periodTimeRemaining - fill with mean or 0\n","    if \"periodTimeRemaining\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.withColumn(\n","            \"periodTimeRemaining\",\n","            when(isnull(col(\"periodTimeRemaining\")), 0).otherwise(col(\"periodTimeRemaining\"))\n","        )\n","\n","    # 2. Fix data types\n","    if \"play_id\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.withColumn(\"play_id\", col(\"play_id\").cast(StringType()))\n","    if \"game_id\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.withColumn(\"game_id\", col(\"game_id\").cast(StringType()))\n","    if \"period\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.withColumn(\"period\", col(\"period\").cast(IntegerType()))\n","    if \"periodTime\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.withColumn(\"periodTime\", col(\"periodTime\").cast(IntegerType()))\n","    if \"dateTime\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.withColumn(\"dateTime\", col(\"dateTime\").cast(TimestampType()))\n","    coord_cols += [\"goals_away\", \"goals_home\"]\n","    for col_name in coord_cols:\n","        if col_name in df_cleaned.columns:\n","            df_cleaned = df_cleaned.withColumn(col_name, col(col_name).cast(FloatType()))\n","    print(\"Schema after type casting:\")\n","    df_cleaned.printSchema()\n","    \n","    # 3.1: Check for exact duplicates (all columns identical)\n","    print(\"\\n=== Checking for Exact Duplicates ===\")\n","    # Count occurrences of each row\n","    duplicate_counts = df_cleaned.groupBy(df_cleaned.columns).count().filter(col(\"count\") > 1)\n","    duplicate_count = duplicate_counts.count()\n","    if duplicate_count > 0:\n","        print(f\"Found {duplicate_count} groups of exact duplicate rows\")\n","        print(\"Sample of exact duplicate rows:\")\n","        duplicate_counts.show(5, truncate=False)\n","    else:\n","        print(\"No exact duplicate rows found\")\n","    # Decision: Drop exact duplicates (exact replicas likely errors)\n","    original_count = df_cleaned.count()\n","    df_cleaned = df_cleaned.dropDuplicates()\n","    print(f\"Removed {original_count - df_cleaned.count()} exact duplicate rows\")\n","\n","    # 3.2: Check for play_id duplicates (non-exact duplicates)\n","    print(\"\\n=== Checking for play_id Duplicates ===\")\n","    # Count occurrences of each play_id\n","    play_id_counts = df_cleaned.groupBy(\"play_id\").count().filter(col(\"count\") > 1)\n","    play_id_duplicate_count = play_id_counts.count()\n","    if play_id_duplicate_count > 0:\n","        print(f\"Found {play_id_duplicate_count} play_id duplicates\")\n","        print(\"Sample of play_id duplicate rows:\")\n","        # Show the rows for duplicate play_ids\n","        duplicate_play_ids = play_id_counts.select(\"play_id\")\n","        duplicate_rows = df_cleaned.join(duplicate_play_ids, \"play_id\", \"inner\")\n","        duplicate_rows.show(5, truncate=False)\n","    else:\n","        print(\"No play_id duplicate rows found\")\n","    # Decision: Drop play_id duplicates, keeping first occurrence\n","    original_count = df_cleaned.count()\n","    df_cleaned = df_cleaned.dropDuplicates([\"play_id\"])\n","    new_count = df_cleaned.count()\n","    print(f\"Removed {original_count - new_count} play_id duplicate rows (keeping first occurrence)\")\n","    original_count = new_count  # Update count for subsequent steps\n","    \n","    # 4. Address imbalance in periodType - just log distribution for now\n","    if \"periodType\" in df_cleaned.columns:\n","        print(\"\\nPeriod Type distribution:\")\n","        df_cleaned.groupBy(\"periodType\").count().show()\n","    \n","    # 5. Validate play_id format (should be like \"YYYYMMDDGG_N\" based on report)\n","    if \"play_id\" in df_cleaned.columns:\n","        df_cleaned = df_cleaned.withColumn(\n","            \"play_id_valid\",\n","            when(col(\"play_id\").rlike(\"^[0-9]{10}_[0-9]+$\"), True).otherwise(False)\n","        )\n","        invalid_play_ids = df_cleaned.filter(~col(\"play_id_valid\")).count()\n","        print(f\"Found {invalid_play_ids} rows with invalid play_id format\")\n","        df_cleaned = df_cleaned.drop(\"play_id_valid\")  # Drop temporary column\n","    \n","    # Print cleaned DataFrame info\n","    print(f\"\\nCleaned DataFrame:\")\n","    print(f\"Row count: {df_cleaned.count()}\")\n","    print(\"\\nCleaned sample data:\")\n","    df_cleaned.show(5, truncate=False)\n","    \n","    # Check for null values after cleaning\n","    print(\"\\nNull value check after cleaning:\")\n","    null_counts = df_cleaned.select([pyspark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_cleaned.columns])\n","    null_counts.show()\n","    \n","    # Save cleaned DataFrame to Lakehouse (Silver layer)\n","    (\n","        df_cleaned.write\n","        .mode(\"overwrite\")\n","        .saveAsTable(\"silver_plays\")\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"da2a3071-7a3a-447f-a291-b6cd5a33eeb0"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"4ac82dab-8c4c-49b9-af13-11310a7aa46e"}],"default_lakehouse":"4ac82dab-8c4c-49b9-af13-11310a7aa46e","default_lakehouse_name":"NHL_Lakehouse","default_lakehouse_workspace_id":"6dbe98ce-44a9-4fc9-8703-ad6aa606dba8"}}},"nbformat":4,"nbformat_minor":5}